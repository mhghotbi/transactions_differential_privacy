"""
Comprehensive DP Evaluation Cells for Jupyter Notebook
Copy these into the notebook as new cells after cell 17 (cleanup section)
"""

# =============================================================================
# CELL: Markdown - Section Header
# =============================================================================
CELL_HEADER = """
## Step 6: Comprehensive Fidelity & Utility Evaluation

Scientific evaluation of DP-protected data quality using rigorous statistical methods.

### Evaluation Framework:

**A. FIDELITY METRICS** (How similar is protected data to original?)
1. Statistical Fidelity - Moments, quantiles, range
2. Distributional Fidelity - KS, JS divergence, Wasserstein, TV distance
3. Structural Fidelity - Correlation/covariance preservation
4. Per-Cell Error Analysis - Error distribution, magnitude effects

**B. UTILITY METRICS** (How useful is protected data for analysis?)
1. Query Accuracy - Point, range, marginal queries
2. Ranking Preservation - Spearman, Kendall, Top-K
3. Analytical Tasks - Hotspots, trends, aggregations
4. Workload-Based Evaluation - Real analytical queries
"""

# =============================================================================
# CELL: Code - Setup and Data Loading
# =============================================================================
CELL_SETUP = '''
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
from scipy import stats
from scipy.spatial.distance import jensenshannon
from scipy.stats import entropy, spearmanr, kendalltau, pearsonr
from collections import defaultdict
import json

# Check if we have valid results
if not result['success']:
    raise RuntimeError("Pipeline failed - cannot evaluate. Check logs above.")

print("="*80)
print("COMPREHENSIVE FIDELITY & UTILITY EVALUATION")
print("="*80)

# Try display import
try:
    from IPython.display import display
except ImportError:
    display = print

# Visualization imports
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False
    print("matplotlib/seaborn not available - skipping visualizations")

print("Setup complete!")
'''

# =============================================================================
# CELL: Code - Load and Prepare Data
# =============================================================================
CELL_LOAD_DATA = '''
# Load original data aggregated to cell level
print("Loading and preparing data...")

# Original aggregated data
original_agg = df.groupby(['acceptor_city', 'mcc', 'transaction_date']).agg({
    'transaction_id': 'count',
    'card_number': 'nunique', 
    'acceptor_id': 'nunique',
    'amount': 'sum'
}).reset_index()
original_agg.columns = ['city', 'mcc', 'date', 'transaction_count', 'unique_cards', 'unique_acceptors', 'total_amount']

# Add province info for hierarchical analysis
city_province = pd.read_csv(CITY_PROVINCE_PATH)
city_to_province = dict(zip(city_province['city'], city_province['province']))
original_agg['province'] = original_agg['city'].map(city_to_province)

print(f"Original aggregated cells: {len(original_agg):,}")

# Load DP-protected data
dp_output_path = config.data.output_path

try:
    parquet_files = [f for f in os.listdir(dp_output_path) if f.endswith('.parquet')]
    if parquet_files:
        dp_agg = pd.read_parquet(os.path.join(dp_output_path, parquet_files[0]))
    else:
        csv_files = [f for f in os.listdir(dp_output_path) if f.endswith('.csv')]
        if csv_files:
            dp_agg = pd.read_csv(os.path.join(dp_output_path, csv_files[0]))
        else:
            raise FileNotFoundError("No output files found")
    
    # Ensure province column exists
    if 'province' not in dp_agg.columns and 'city' in dp_agg.columns:
        dp_agg['province'] = dp_agg['city'].map(city_to_province)
    
    print(f"DP-protected cells: {len(dp_agg):,}")
    DATA_LOADED = True
    
except Exception as e:
    print(f"Could not load DP output: {e}")
    print("Simulating DP noise for demonstration...")
    
    # Simulate DP noise
    np.random.seed(42)
    dp_agg = original_agg.copy()
    rho = float(config.privacy.total_rho)
    sigma = np.sqrt(1 / (2 * rho))
    
    for col in ['transaction_count', 'unique_cards', 'unique_acceptors']:
        noise = np.random.normal(0, sigma, len(dp_agg))
        dp_agg[col] = np.maximum(0, np.round(dp_agg[col] + noise)).astype(int)
    
    noise_amount = np.random.normal(0, sigma * dp_agg['total_amount'].std() * 0.01, len(dp_agg))
    dp_agg['total_amount'] = np.maximum(0, dp_agg['total_amount'] + noise_amount)
    dp_agg['province'] = original_agg['province']
    DATA_LOADED = True

# Define numeric columns for analysis
NUMERIC_COLS = ['transaction_count', 'unique_cards', 'unique_acceptors', 'total_amount']
COUNT_COLS = ['transaction_count', 'unique_cards', 'unique_acceptors']

print("\\n‚úÖ Data ready for evaluation!")
print(f"   Columns: {NUMERIC_COLS}")
'''

# =============================================================================
# CELL: Markdown - Part A Header
# =============================================================================
CELL_PART_A_HEADER = """
---
## Part A: FIDELITY METRICS

Measuring how faithfully the DP-protected data represents the original data.
"""

# =============================================================================
# CELL: Code - A1. Statistical Fidelity
# =============================================================================
CELL_A1_STATISTICAL = '''
print("="*80)
print("A1. STATISTICAL FIDELITY")
print("="*80)
print("\\nMeasuring preservation of statistical moments and quantiles.\\n")

def compute_statistical_fidelity(orig, synth, columns):
    """
    Compute comprehensive statistical fidelity metrics.
    
    Metrics:
    - Moments: mean, std, skewness, kurtosis
    - Quantiles: 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%
    - Range: min, max
    - Relative errors for each
    """
    results = {}
    quantile_levels = [0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]
    
    for col in columns:
        o = orig[col].values.astype(float)
        s = synth[col].values.astype(float)
        
        # Moments
        moments = {
            'mean': (np.mean(o), np.mean(s)),
            'std': (np.std(o), np.std(s)),
            'skewness': (stats.skew(o), stats.skew(s)),
            'kurtosis': (stats.kurtosis(o), stats.kurtosis(s)),
            'min': (np.min(o), np.min(s)),
            'max': (np.max(o), np.max(s)),
        }
        
        # Quantiles
        quantiles = {}
        for q in quantile_levels:
            orig_q = np.percentile(o, q * 100)
            synth_q = np.percentile(s, q * 100)
            quantiles[f'q{int(q*100):02d}'] = (orig_q, synth_q)
        
        # Compute relative errors
        errors = {}
        for name, (orig_val, synth_val) in {**moments, **quantiles}.items():
            if abs(orig_val) > 1e-10:
                rel_error = abs(synth_val - orig_val) / abs(orig_val) * 100
            else:
                rel_error = abs(synth_val - orig_val) * 100
            errors[name] = rel_error
        
        results[col] = {
            'moments': moments,
            'quantiles': quantiles,
            'errors': errors,
            'mean_moment_error': np.mean([errors[k] for k in ['mean', 'std', 'skewness', 'kurtosis']]),
            'mean_quantile_error': np.mean([errors[k] for k in quantiles.keys()]),
        }
    
    return results

stat_fidelity = compute_statistical_fidelity(original_agg, dp_agg, NUMERIC_COLS)

# Display results
for col in NUMERIC_COLS:
    res = stat_fidelity[col]
    print(f"\\nüìä {col.upper()}")
    print("-" * 70)
    
    # Moments table
    print("  MOMENTS:")
    print(f"  {'Statistic':<12} {'Original':>15} {'DP-Protected':>15} {'Error %':>10}")
    print("  " + "-"*55)
    for name in ['mean', 'std', 'skewness', 'kurtosis', 'min', 'max']:
        orig_val, synth_val = res['moments'][name]
        error = res['errors'][name]
        status = "‚úÖ" if error < 10 else ("‚ö†Ô∏è" if error < 25 else "‚ùå")
        print(f"  {name:<12} {orig_val:>15,.2f} {synth_val:>15,.2f} {error:>9.1f}% {status}")
    
    print(f"\\n  QUANTILES (Mean Error: {res['mean_quantile_error']:.1f}%):")
    print(f"  {'Percentile':<12} {'Original':>15} {'DP-Protected':>15} {'Error %':>10}")
    print("  " + "-"*55)
    for q_name, (orig_val, synth_val) in res['quantiles'].items():
        error = res['errors'][q_name]
        status = "‚úÖ" if error < 10 else ("‚ö†Ô∏è" if error < 25 else "‚ùå")
        print(f"  {q_name:<12} {orig_val:>15,.2f} {synth_val:>15,.2f} {error:>9.1f}% {status}")

# Summary score
avg_moment_error = np.mean([stat_fidelity[c]['mean_moment_error'] for c in NUMERIC_COLS])
avg_quantile_error = np.mean([stat_fidelity[c]['mean_quantile_error'] for c in NUMERIC_COLS])
stat_fidelity_score = max(0, 100 - (avg_moment_error + avg_quantile_error) / 2)

print(f"\\n{'='*70}")
print(f"STATISTICAL FIDELITY SCORE: {stat_fidelity_score:.1f}/100")
print(f"  - Mean Moment Error: {avg_moment_error:.1f}%")
print(f"  - Mean Quantile Error: {avg_quantile_error:.1f}%")
'''

# =============================================================================
# CELL: Code - A2. Distributional Fidelity
# =============================================================================
CELL_A2_DISTRIBUTIONAL = '''
print("\\n" + "="*80)
print("A2. DISTRIBUTIONAL FIDELITY")
print("="*80)
print("\\nMeasuring similarity of probability distributions.\\n")

def compute_distributional_fidelity(orig, synth, columns):
    """
    Compute distributional similarity metrics.
    
    Metrics:
    - Kolmogorov-Smirnov (KS) statistic and p-value
    - Jensen-Shannon (JS) divergence
    - Wasserstein distance (Earth Mover's Distance)
    - Total Variation (TV) distance
    - Histogram Intersection
    """
    results = {}
    
    for col in columns:
        o = orig[col].values.astype(float)
        s = synth[col].values.astype(float)
        
        # Remove NaN
        o = o[~np.isnan(o)]
        s = s[~np.isnan(s)]
        
        # 1. Kolmogorov-Smirnov test
        ks_stat, ks_pval = stats.ks_2samp(o, s)
        
        # 2. Create histograms for divergence metrics
        min_val = min(o.min(), s.min())
        max_val = max(o.max(), s.max())
        bins = np.linspace(min_val, max_val, 100)
        
        o_hist, _ = np.histogram(o, bins=bins, density=True)
        s_hist, _ = np.histogram(s, bins=bins, density=True)
        
        # Normalize and add epsilon
        o_hist = (o_hist + 1e-10) / (o_hist + 1e-10).sum()
        s_hist = (s_hist + 1e-10) / (s_hist + 1e-10).sum()
        
        # 3. Jensen-Shannon divergence
        js_div = jensenshannon(o_hist, s_hist)
        
        # 4. Total Variation distance
        tv_dist = 0.5 * np.sum(np.abs(o_hist - s_hist))
        
        # 5. Histogram Intersection (similarity, not distance)
        hist_intersection = np.sum(np.minimum(o_hist, s_hist))
        
        # 6. Wasserstein distance
        wasserstein = stats.wasserstein_distance(o, s)
        wasserstein_norm = wasserstein / (max_val - min_val + 1e-10)
        
        # 7. Hellinger distance
        hellinger = np.sqrt(0.5 * np.sum((np.sqrt(o_hist) - np.sqrt(s_hist))**2))
        
        results[col] = {
            'ks_statistic': ks_stat,
            'ks_pvalue': ks_pval,
            'js_divergence': js_div,
            'js_similarity': 1 - js_div,
            'tv_distance': tv_dist,
            'tv_similarity': 1 - tv_dist,
            'histogram_intersection': hist_intersection,
            'wasserstein_distance': wasserstein,
            'wasserstein_normalized': wasserstein_norm,
            'hellinger_distance': hellinger,
            'hellinger_similarity': 1 - hellinger,
        }
    
    return results

dist_fidelity = compute_distributional_fidelity(original_agg, dp_agg, NUMERIC_COLS)

# Display results
print(f"{'Column':<20} {'KS Stat':>10} {'KS p-val':>10} {'JS Div':>10} {'TV Dist':>10} {'Hist Int':>10} {'Hellinger':>10}")
print("-" * 85)

for col in NUMERIC_COLS:
    r = dist_fidelity[col]
    ks_status = "‚úÖ" if r['ks_pvalue'] > 0.05 else "‚ö†Ô∏è"
    print(f"{col:<20} {r['ks_statistic']:>10.4f} {r['ks_pvalue']:>9.4f}{ks_status} {r['js_divergence']:>10.4f} {r['tv_distance']:>10.4f} {r['histogram_intersection']:>10.4f} {r['hellinger_distance']:>10.4f}")

# Detailed breakdown
print("\\n" + "="*70)
print("INTERPRETATION:")
print("-"*70)
for col in NUMERIC_COLS:
    r = dist_fidelity[col]
    print(f"\\n{col}:")
    
    # KS interpretation
    if r['ks_pvalue'] > 0.05:
        print(f"  ‚úÖ KS Test: Distributions are statistically similar (p={r['ks_pvalue']:.4f})")
    else:
        print(f"  ‚ö†Ô∏è KS Test: Distributions differ significantly (p={r['ks_pvalue']:.4f})")
    
    # JS interpretation (0=identical, 1=completely different)
    js_pct = r['js_similarity'] * 100
    print(f"  {'‚úÖ' if js_pct > 80 else '‚ö†Ô∏è' if js_pct > 60 else '‚ùå'} Jensen-Shannon Similarity: {js_pct:.1f}%")
    
    # TV interpretation
    tv_pct = r['tv_similarity'] * 100
    print(f"  {'‚úÖ' if tv_pct > 80 else '‚ö†Ô∏è' if tv_pct > 60 else '‚ùå'} Total Variation Similarity: {tv_pct:.1f}%")
    
    # Histogram intersection
    hi_pct = r['histogram_intersection'] * 100
    print(f"  {'‚úÖ' if hi_pct > 80 else '‚ö†Ô∏è' if hi_pct > 60 else '‚ùå'} Histogram Intersection: {hi_pct:.1f}%")

# Compute overall score
dist_scores = []
for col in NUMERIC_COLS:
    r = dist_fidelity[col]
    col_score = (r['js_similarity'] + r['tv_similarity'] + r['histogram_intersection'] + r['hellinger_similarity']) / 4 * 100
    dist_scores.append(col_score)

dist_fidelity_score = np.mean(dist_scores)
print(f"\\n{'='*70}")
print(f"DISTRIBUTIONAL FIDELITY SCORE: {dist_fidelity_score:.1f}/100")
'''

# =============================================================================
# CELL: Code - A3. Structural Fidelity
# =============================================================================
CELL_A3_STRUCTURAL = '''
print("\\n" + "="*80)
print("A3. STRUCTURAL FIDELITY")
print("="*80)
print("\\nMeasuring preservation of relationships between variables.\\n")

def compute_structural_fidelity(orig, synth, columns):
    """
    Compute structural fidelity metrics.
    
    Metrics:
    - Pearson correlation matrix preservation
    - Spearman correlation matrix preservation
    - Covariance matrix difference (Frobenius norm)
    - Pairwise correlation comparison
    """
    # Correlation matrices
    orig_pearson = orig[columns].corr(method='pearson')
    synth_pearson = synth[columns].corr(method='pearson')
    
    orig_spearman = orig[columns].corr(method='spearman')
    synth_spearman = synth[columns].corr(method='spearman')
    
    # Covariance matrices
    orig_cov = orig[columns].cov()
    synth_cov = synth[columns].cov()
    
    # Frobenius norm of difference
    pearson_diff = np.linalg.norm(orig_pearson.values - synth_pearson.values, 'fro')
    spearman_diff = np.linalg.norm(orig_spearman.values - synth_spearman.values, 'fro')
    cov_diff = np.linalg.norm(orig_cov.values - synth_cov.values, 'fro')
    cov_norm = np.linalg.norm(orig_cov.values, 'fro')
    
    # Pairwise correlation comparison
    pairwise = []
    n = len(columns)
    for i in range(n):
        for j in range(i+1, n):
            c1, c2 = columns[i], columns[j]
            pairwise.append({
                'pair': f"{c1[:8]}..{c2[:8]}",
                'orig_pearson': orig_pearson.loc[c1, c2],
                'synth_pearson': synth_pearson.loc[c1, c2],
                'orig_spearman': orig_spearman.loc[c1, c2],
                'synth_spearman': synth_spearman.loc[c1, c2],
            })
    
    # Correlation of correlations (meta-correlation)
    orig_corrs = [p['orig_pearson'] for p in pairwise]
    synth_corrs = [p['synth_pearson'] for p in pairwise]
    if len(orig_corrs) > 1:
        corr_of_corrs, _ = pearsonr(orig_corrs, synth_corrs)
    else:
        corr_of_corrs = 1.0
    
    # Mean absolute correlation error
    mean_corr_error = np.mean([abs(p['orig_pearson'] - p['synth_pearson']) for p in pairwise])
    
    return {
        'orig_pearson': orig_pearson,
        'synth_pearson': synth_pearson,
        'orig_spearman': orig_spearman,
        'synth_spearman': synth_spearman,
        'pearson_frobenius_diff': pearson_diff,
        'spearman_frobenius_diff': spearman_diff,
        'cov_frobenius_diff': cov_diff,
        'cov_frobenius_norm': cov_norm,
        'cov_relative_diff': cov_diff / (cov_norm + 1e-10),
        'correlation_of_correlations': corr_of_corrs,
        'mean_correlation_error': mean_corr_error,
        'pairwise': pairwise,
    }

struct_fidelity = compute_structural_fidelity(original_agg, dp_agg, NUMERIC_COLS)

# Display correlation matrices
print("PEARSON CORRELATION MATRICES")
print("-"*70)
print("\\nOriginal:")
display(struct_fidelity['orig_pearson'].round(4))
print("\\nDP-Protected:")
display(struct_fidelity['synth_pearson'].round(4))

# Pairwise comparison
print("\\n" + "-"*70)
print("PAIRWISE CORRELATION COMPARISON")
print("-"*70)
print(f"{'Pair':<20} {'Orig Pearson':>14} {'DP Pearson':>14} {'Diff':>10} {'Status':>8}")
print("-"*70)

for p in struct_fidelity['pairwise']:
    diff = abs(p['orig_pearson'] - p['synth_pearson'])
    status = "‚úÖ" if diff < 0.1 else ("‚ö†Ô∏è" if diff < 0.2 else "‚ùå")
    print(f"{p['pair']:<20} {p['orig_pearson']:>14.4f} {p['synth_pearson']:>14.4f} {diff:>10.4f} {status:>8}")

# Summary metrics
print("\\n" + "="*70)
print("STRUCTURAL FIDELITY SUMMARY")
print("-"*70)
print(f"  Correlation of Correlations: {struct_fidelity['correlation_of_correlations']:.4f}")
print(f"  Mean Correlation Error: {struct_fidelity['mean_correlation_error']:.4f}")
print(f"  Pearson Matrix Frobenius Diff: {struct_fidelity['pearson_frobenius_diff']:.4f}")
print(f"  Covariance Relative Diff: {struct_fidelity['cov_relative_diff']:.4f}")

# Score
struct_fidelity_score = max(0, struct_fidelity['correlation_of_correlations'] * 100)
if np.isnan(struct_fidelity_score):
    struct_fidelity_score = 50.0

print(f"\\n{'='*70}")
print(f"STRUCTURAL FIDELITY SCORE: {struct_fidelity_score:.1f}/100")
'''

# =============================================================================
# CELL: Code - A4. Per-Cell Error Analysis
# =============================================================================
CELL_A4_PERCELL = '''
print("\\n" + "="*80)
print("A4. PER-CELL ERROR ANALYSIS (DP-Specific)")
print("="*80)
print("\\nAnalyzing error distribution across individual cells.\\n")

def compute_percell_errors(orig, synth, columns):
    """
    Compute per-cell error metrics critical for DP evaluation.
    
    Metrics:
    - Absolute errors (distribution)
    - Relative errors (distribution)
    - Error vs magnitude correlation
    - Sign flip rate (how many positives became negative before clipping)
    - Zero inflation (how many non-zeros became zero)
    """
    results = {}
    
    for col in columns:
        o = orig[col].values.astype(float)
        s = synth[col].values.astype(float)
        
        # Absolute errors
        abs_errors = np.abs(s - o)
        
        # Relative errors (avoiding division by zero)
        with np.errstate(divide='ignore', invalid='ignore'):
            rel_errors = np.abs(s - o) / np.abs(o)
            rel_errors[~np.isfinite(rel_errors)] = np.nan
        rel_errors_valid = rel_errors[~np.isnan(rel_errors)]
        
        # Signed errors (for bias detection)
        signed_errors = s - o
        
        # Error vs magnitude correlation
        if len(o) > 2:
            magnitude_corr, _ = spearmanr(np.abs(o), abs_errors)
        else:
            magnitude_corr = 0.0
        
        # Sign flip rate (positive to zero or negative)
        positives = o > 0
        sign_flips = positives & (s <= 0)
        sign_flip_rate = np.sum(sign_flips) / (np.sum(positives) + 1e-10) * 100
        
        # Zero inflation
        nonzeros_orig = o != 0
        zeros_synth = s == 0
        zero_inflation = np.sum(nonzeros_orig & zeros_synth) / (np.sum(nonzeros_orig) + 1e-10) * 100
        
        # Error quantiles
        error_quantiles = {
            'p50': np.percentile(abs_errors, 50),
            'p90': np.percentile(abs_errors, 90),
            'p95': np.percentile(abs_errors, 95),
            'p99': np.percentile(abs_errors, 99),
            'max': np.max(abs_errors),
        }
        
        results[col] = {
            'mean_abs_error': np.mean(abs_errors),
            'std_abs_error': np.std(abs_errors),
            'median_abs_error': np.median(abs_errors),
            'mean_rel_error': np.nanmean(rel_errors_valid) * 100 if len(rel_errors_valid) > 0 else 0,
            'median_rel_error': np.nanmedian(rel_errors_valid) * 100 if len(rel_errors_valid) > 0 else 0,
            'bias': np.mean(signed_errors),
            'magnitude_error_corr': magnitude_corr,
            'sign_flip_rate': sign_flip_rate,
            'zero_inflation_rate': zero_inflation,
            'error_quantiles': error_quantiles,
            'abs_errors': abs_errors,
            'rel_errors': rel_errors_valid,
        }
    
    return results

percell_errors = compute_percell_errors(original_agg, dp_agg, NUMERIC_COLS)

# Display results
for col in NUMERIC_COLS:
    r = percell_errors[col]
    print(f"\\nüìä {col.upper()}")
    print("-"*70)
    
    print("  ABSOLUTE ERROR:")
    print(f"    Mean: {r['mean_abs_error']:,.2f}")
    print(f"    Median: {r['median_abs_error']:,.2f}")
    print(f"    Std: {r['std_abs_error']:,.2f}")
    print(f"    P90: {r['error_quantiles']['p90']:,.2f}")
    print(f"    P99: {r['error_quantiles']['p99']:,.2f}")
    print(f"    Max: {r['error_quantiles']['max']:,.2f}")
    
    print("\\n  RELATIVE ERROR:")
    print(f"    Mean: {r['mean_rel_error']:.1f}%")
    print(f"    Median: {r['median_rel_error']:.1f}%")
    
    print("\\n  BIAS & DISTORTION:")
    bias_status = "‚úÖ" if abs(r['bias']) < r['std_abs_error'] * 0.1 else "‚ö†Ô∏è"
    print(f"    Bias (mean signed error): {r['bias']:,.2f} {bias_status}")
    print(f"    Error-Magnitude Correlation: {r['magnitude_error_corr']:.4f}")
    
    print("\\n  DATA DISTORTION:")
    sf_status = "‚úÖ" if r['sign_flip_rate'] < 5 else ("‚ö†Ô∏è" if r['sign_flip_rate'] < 15 else "‚ùå")
    zi_status = "‚úÖ" if r['zero_inflation_rate'] < 5 else ("‚ö†Ô∏è" if r['zero_inflation_rate'] < 15 else "‚ùå")
    print(f"    Sign Flip Rate: {r['sign_flip_rate']:.1f}% {sf_status}")
    print(f"    Zero Inflation Rate: {r['zero_inflation_rate']:.1f}% {zi_status}")

# Score based on relative error and distortion
percell_scores = []
for col in NUMERIC_COLS:
    r = percell_errors[col]
    # Lower relative error is better
    rel_score = max(0, 100 - r['median_rel_error'])
    # Lower distortion is better
    distort_score = max(0, 100 - r['sign_flip_rate'] - r['zero_inflation_rate'])
    percell_scores.append((rel_score + distort_score) / 2)

percell_fidelity_score = np.mean(percell_scores)
print(f"\\n{'='*70}")
print(f"PER-CELL ERROR SCORE: {percell_fidelity_score:.1f}/100")
'''

# =============================================================================
# CELL: Code - A5. Stratified Error Analysis
# =============================================================================
CELL_A5_STRATIFIED = '''
print("\\n" + "="*80)
print("A5. STRATIFIED ERROR ANALYSIS BY MAGNITUDE")
print("="*80)
print("\\nAnalyzing how error varies with count magnitude (critical for DP!).\\n")

def stratified_error_analysis(orig, synth, col, n_strata=5):
    """
    Analyze errors stratified by original value magnitude.
    
    DP noise has constant variance, so relative error is worse for small counts.
    This analysis quantifies that effect.
    """
    o = orig[col].values.astype(float)
    s = synth[col].values.astype(float)
    
    # Create strata based on original values
    if o.min() == o.max():
        return None
    
    # Use quantiles for stratification
    try:
        strata_edges = np.percentile(o[o > 0], np.linspace(0, 100, n_strata + 1))
        strata_edges = np.unique(strata_edges)  # Remove duplicates
    except:
        return None
    
    if len(strata_edges) < 2:
        return None
    
    results = []
    for i in range(len(strata_edges) - 1):
        low, high = strata_edges[i], strata_edges[i+1]
        mask = (o >= low) & (o < high if i < len(strata_edges) - 2 else o <= high)
        
        if np.sum(mask) < 10:
            continue
        
        o_stratum = o[mask]
        s_stratum = s[mask]
        
        abs_errors = np.abs(s_stratum - o_stratum)
        rel_errors = abs_errors / (o_stratum + 1e-10) * 100
        
        results.append({
            'range': f"{low:,.0f}-{high:,.0f}",
            'count': np.sum(mask),
            'mean_orig': np.mean(o_stratum),
            'mean_abs_error': np.mean(abs_errors),
            'mean_rel_error': np.mean(rel_errors),
            'median_rel_error': np.median(rel_errors),
        })
    
    return results

# Analyze each column
for col in NUMERIC_COLS:
    print(f"\\nüìä {col.upper()}")
    print("-"*80)
    
    strata = stratified_error_analysis(original_agg, dp_agg, col)
    
    if strata is None:
        print("  Insufficient data for stratified analysis")
        continue
    
    print(f"  {'Value Range':<20} {'N Cells':>10} {'Mean Value':>15} {'Mean Abs Err':>15} {'Mean Rel Err':>12}")
    print("  " + "-"*75)
    
    for s in strata:
        status = "‚úÖ" if s['mean_rel_error'] < 25 else ("‚ö†Ô∏è" if s['mean_rel_error'] < 50 else "‚ùå")
        print(f"  {s['range']:<20} {s['count']:>10,} {s['mean_orig']:>15,.1f} {s['mean_abs_error']:>15,.1f} {s['mean_rel_error']:>11.1f}% {status}")

print("\\n" + "="*70)
print("NOTE: DP noise has constant variance (œÉ), so relative error = œÉ/value")
print("      Small counts will always have higher relative error - this is expected!")
print("      The key is whether ABSOLUTE error is appropriate for the privacy budget.")
'''

# =============================================================================
# CELL: Markdown - Part B Header
# =============================================================================
CELL_PART_B_HEADER = """
---
## Part B: UTILITY METRICS

Measuring how useful the DP-protected data is for analytical tasks.
"""

# =============================================================================
# CELL: Code - B1. Query Accuracy
# =============================================================================
CELL_B1_QUERY = '''
print("="*80)
print("B1. QUERY ACCURACY")
print("="*80)
print("\\nMeasuring accuracy of different query types.\\n")

def compute_query_accuracy(orig, synth, columns, group_cols=['city', 'mcc']):
    """
    Compute accuracy for various query types.
    
    Query types:
    - Point queries (individual cell accuracy)
    - Marginal queries (1-way aggregations)
    - Range queries (2-way aggregations)
    - Total queries (global aggregations)
    """
    results = {}
    
    for col in columns:
        o = orig[col].values.astype(float)
        s = synth[col].values.astype(float)
        
        # 1. Point query metrics
        abs_errors = np.abs(s - o)
        rel_errors = abs_errors / (np.abs(o) + 1e-10) * 100
        
        # RMSE
        rmse = np.sqrt(np.mean(abs_errors**2))
        
        # MAPE (Mean Absolute Percentage Error)
        mape = np.mean(rel_errors[o > 0])  # Only for non-zero cells
        
        # SMAPE (Symmetric MAPE)
        smape = np.mean(2 * abs_errors / (np.abs(o) + np.abs(s) + 1e-10)) * 100
        
        results[col] = {
            'point_rmse': rmse,
            'point_mape': mape,
            'point_smape': smape,
            'point_mae': np.mean(abs_errors),
        }
        
        # 2. Marginal queries (by city)
        if 'city' in orig.columns:
            orig_by_city = orig.groupby('city')[col].sum()
            synth_by_city = synth.groupby('city')[col].sum()
            common = orig_by_city.index.intersection(synth_by_city.index)
            
            if len(common) > 0:
                o_city = orig_by_city.loc[common].values
                s_city = synth_by_city.loc[common].values
                
                city_errors = np.abs(s_city - o_city)
                city_rel_errors = city_errors / (o_city + 1e-10) * 100
                
                results[col]['marginal_city_mae'] = np.mean(city_errors)
                results[col]['marginal_city_mape'] = np.mean(city_rel_errors[o_city > 0])
        
        # 3. Marginal queries (by MCC)
        if 'mcc' in orig.columns:
            orig_by_mcc = orig.groupby('mcc')[col].sum()
            synth_by_mcc = synth.groupby('mcc')[col].sum()
            common = orig_by_mcc.index.intersection(synth_by_mcc.index)
            
            if len(common) > 0:
                o_mcc = orig_by_mcc.loc[common].values
                s_mcc = synth_by_mcc.loc[common].values
                
                mcc_errors = np.abs(s_mcc - o_mcc)
                mcc_rel_errors = mcc_errors / (o_mcc + 1e-10) * 100
                
                results[col]['marginal_mcc_mae'] = np.mean(mcc_errors)
                results[col]['marginal_mcc_mape'] = np.mean(mcc_rel_errors[o_mcc > 0])
        
        # 4. Total queries
        orig_total = np.sum(o)
        synth_total = np.sum(s)
        results[col]['total_error'] = abs(synth_total - orig_total)
        results[col]['total_error_pct'] = abs(synth_total - orig_total) / (orig_total + 1e-10) * 100
        results[col]['orig_total'] = orig_total
        results[col]['synth_total'] = synth_total
    
    return results

query_accuracy = compute_query_accuracy(original_agg, dp_agg, NUMERIC_COLS)

# Display results
print("POINT QUERY ACCURACY (Individual Cells)")
print("-"*70)
print(f"{'Column':<25} {'RMSE':>12} {'MAE':>12} {'MAPE':>10} {'SMAPE':>10}")
print("-"*70)
for col in NUMERIC_COLS:
    r = query_accuracy[col]
    print(f"{col:<25} {r['point_rmse']:>12,.2f} {r['point_mae']:>12,.2f} {r['point_mape']:>9.1f}% {r['point_smape']:>9.1f}%")

print("\\nMARGINAL QUERY ACCURACY (Aggregated)")
print("-"*70)
print(f"{'Column':<25} {'City MAPE':>12} {'MCC MAPE':>12}")
print("-"*70)
for col in NUMERIC_COLS:
    r = query_accuracy[col]
    city_mape = r.get('marginal_city_mape', np.nan)
    mcc_mape = r.get('marginal_mcc_mape', np.nan)
    print(f"{col:<25} {city_mape:>11.1f}% {mcc_mape:>11.1f}%")

print("\\nTOTAL QUERY ACCURACY (Global Sums)")
print("-"*70)
print(f"{'Column':<25} {'Original Total':>18} {'DP Total':>18} {'Error %':>10}")
print("-"*70)
for col in NUMERIC_COLS:
    r = query_accuracy[col]
    status = "‚úÖ" if r['total_error_pct'] < 5 else ("‚ö†Ô∏è" if r['total_error_pct'] < 15 else "‚ùå")
    print(f"{col:<25} {r['orig_total']:>18,.0f} {r['synth_total']:>18,.0f} {r['total_error_pct']:>9.1f}% {status}")

# Score
query_scores = []
for col in NUMERIC_COLS:
    r = query_accuracy[col]
    # Combine point and marginal accuracy
    point_score = max(0, 100 - r['point_mape'])
    total_score = max(0, 100 - r['total_error_pct'] * 5)  # Weight total accuracy higher
    query_scores.append((point_score + total_score) / 2)

query_accuracy_score = np.mean(query_scores)
print(f"\\n{'='*70}")
print(f"QUERY ACCURACY SCORE: {query_accuracy_score:.1f}/100")
'''

# =============================================================================
# CELL: Code - B2. Ranking Preservation
# =============================================================================
CELL_B2_RANKING = '''
print("\\n" + "="*80)
print("B2. RANKING PRESERVATION")
print("="*80)
print("\\nMeasuring how well rankings are preserved after DP.\\n")

def compute_ranking_metrics(orig, synth, columns, group_col='city', top_k_values=[5, 10, 20]):
    """
    Compute ranking preservation metrics.
    
    Metrics:
    - Spearman rank correlation
    - Kendall tau
    - Top-K overlap (Jaccard similarity)
    - Precision@K and Recall@K
    - Rank displacement distribution
    """
    results = {}
    
    for col in columns:
        col_results = {}
        
        if group_col in orig.columns:
            # Aggregate by group
            orig_agg = orig.groupby(group_col)[col].sum().sort_values(ascending=False)
            synth_agg = synth.groupby(group_col)[col].sum()
            
            # Align indices
            common = orig_agg.index.intersection(synth_agg.index)
            if len(common) < 3:
                continue
                
            o = orig_agg.loc[common]
            s = synth_agg.loc[common]
            
            # Spearman correlation
            spearman_corr, spearman_p = spearmanr(o, s)
            col_results['spearman'] = spearman_corr
            col_results['spearman_pvalue'] = spearman_p
            
            # Kendall tau
            kendall_corr, kendall_p = kendalltau(o, s)
            col_results['kendall'] = kendall_corr
            col_results['kendall_pvalue'] = kendall_p
            
            # Top-K metrics
            col_results['top_k'] = {}
            for k in top_k_values:
                if k > len(common):
                    continue
                
                orig_top_k = set(o.nlargest(k).index)
                synth_top_k = set(s.nlargest(k).index)
                
                # Jaccard similarity
                intersection = len(orig_top_k & synth_top_k)
                union = len(orig_top_k | synth_top_k)
                jaccard = intersection / union if union > 0 else 0
                
                # Precision and Recall (treating original as ground truth)
                precision = intersection / k
                recall = intersection / k  # Same since both sets have size k
                
                col_results['top_k'][k] = {
                    'jaccard': jaccard,
                    'precision': precision,
                    'recall': recall,
                    'overlap_count': intersection,
                }
            
            # Rank displacement
            orig_ranks = o.rank(ascending=False)
            synth_ranks = s.rank(ascending=False)
            rank_displacement = np.abs(orig_ranks - synth_ranks.loc[orig_ranks.index])
            
            col_results['mean_rank_displacement'] = rank_displacement.mean()
            col_results['max_rank_displacement'] = rank_displacement.max()
            col_results['median_rank_displacement'] = rank_displacement.median()
        
        results[col] = col_results
    
    return results

# Compute for city aggregation
ranking_by_city = compute_ranking_metrics(original_agg, dp_agg, NUMERIC_COLS, 'city')

# Compute for MCC aggregation
ranking_by_mcc = compute_ranking_metrics(original_agg, dp_agg, NUMERIC_COLS, 'mcc')

# Display results
print("RANKING BY CITY")
print("-"*80)
print(f"{'Column':<25} {'Spearman':>10} {'Kendall':>10} {'Top-5':>10} {'Top-10':>10} {'Rank Disp':>12}")
print("-"*80)
for col in NUMERIC_COLS:
    if col not in ranking_by_city:
        continue
    r = ranking_by_city[col]
    sp = r.get('spearman', np.nan)
    kt = r.get('kendall', np.nan)
    t5 = r.get('top_k', {}).get(5, {}).get('precision', np.nan) * 100
    t10 = r.get('top_k', {}).get(10, {}).get('precision', np.nan) * 100
    rd = r.get('mean_rank_displacement', np.nan)
    
    sp_status = "‚úÖ" if sp > 0.9 else ("‚ö†Ô∏è" if sp > 0.7 else "‚ùå")
    print(f"{col:<25} {sp:>9.4f}{sp_status} {kt:>10.4f} {t5:>9.0f}% {t10:>9.0f}% {rd:>12.1f}")

print("\\nRANKING BY MCC")
print("-"*80)
print(f"{'Column':<25} {'Spearman':>10} {'Kendall':>10} {'Top-5':>10} {'Top-10':>10} {'Rank Disp':>12}")
print("-"*80)
for col in NUMERIC_COLS:
    if col not in ranking_by_mcc:
        continue
    r = ranking_by_mcc[col]
    sp = r.get('spearman', np.nan)
    kt = r.get('kendall', np.nan)
    t5 = r.get('top_k', {}).get(5, {}).get('precision', np.nan) * 100
    t10 = r.get('top_k', {}).get(10, {}).get('precision', np.nan) * 100
    rd = r.get('mean_rank_displacement', np.nan)
    
    sp_status = "‚úÖ" if sp > 0.9 else ("‚ö†Ô∏è" if sp > 0.7 else "‚ùå")
    print(f"{col:<25} {sp:>9.4f}{sp_status} {kt:>10.4f} {t5:>9.0f}% {t10:>9.0f}% {rd:>12.1f}")

# Score
ranking_scores = []
for col in NUMERIC_COLS:
    if col in ranking_by_city:
        sp = ranking_by_city[col].get('spearman', 0)
        ranking_scores.append(max(0, sp * 100))
    if col in ranking_by_mcc:
        sp = ranking_by_mcc[col].get('spearman', 0)
        ranking_scores.append(max(0, sp * 100))

ranking_score = np.mean(ranking_scores) if ranking_scores else 50.0
print(f"\\n{'='*70}")
print(f"RANKING PRESERVATION SCORE: {ranking_score:.1f}/100")
'''

# =============================================================================
# CELL: Code - B3. Temporal Pattern Preservation
# =============================================================================
CELL_B3_TEMPORAL = '''
print("\\n" + "="*80)
print("B3. TEMPORAL PATTERN PRESERVATION")
print("="*80)
print("\\nMeasuring preservation of time-series patterns.\\n")

def compute_temporal_metrics(orig, synth, columns, date_col='date'):
    """
    Compute temporal pattern preservation metrics.
    
    Metrics:
    - Daily total correlation
    - Trend preservation (slope similarity)
    - Day-of-week pattern preservation
    - Peak day detection accuracy
    """
    results = {}
    
    if date_col not in orig.columns:
        return results
    
    for col in columns:
        col_results = {}
        
        # Daily aggregates
        orig_daily = orig.groupby(date_col)[col].sum().sort_index()
        synth_daily = synth.groupby(date_col)[col].sum().sort_index()
        
        common_dates = orig_daily.index.intersection(synth_daily.index)
        if len(common_dates) < 3:
            continue
        
        o_daily = orig_daily.loc[common_dates].values
        s_daily = synth_daily.loc[common_dates].values
        
        # Daily correlation
        if len(o_daily) > 2:
            daily_corr, _ = pearsonr(o_daily, s_daily)
        else:
            daily_corr = np.nan
        col_results['daily_correlation'] = daily_corr
        
        # Trend preservation (linear regression slopes)
        x = np.arange(len(o_daily))
        if len(x) > 2:
            orig_slope, _, _, _, _ = stats.linregress(x, o_daily)
            synth_slope, _, _, _, _ = stats.linregress(x, s_daily)
            slope_diff = abs(orig_slope - synth_slope) / (abs(orig_slope) + 1e-10)
            col_results['trend_slope_diff'] = slope_diff
            col_results['trend_preserved'] = slope_diff < 0.5  # Within 50%
        
        # Day-of-week pattern (if dates are parseable)
        try:
            dates = pd.to_datetime(common_dates)
            dow_orig = pd.Series(o_daily, index=dates).groupby(dates.dayofweek).mean()
            dow_synth = pd.Series(s_daily, index=dates).groupby(dates.dayofweek).mean()
            
            if len(dow_orig) > 2:
                dow_corr, _ = pearsonr(dow_orig, dow_synth)
                col_results['dow_correlation'] = dow_corr
        except:
            pass
        
        # Peak detection (top 3 days)
        k = min(3, len(o_daily))
        orig_peaks = set(np.argsort(o_daily)[-k:])
        synth_peaks = set(np.argsort(s_daily)[-k:])
        peak_overlap = len(orig_peaks & synth_peaks) / k
        col_results['peak_detection_accuracy'] = peak_overlap
        
        results[col] = col_results
    
    return results

temporal_metrics = compute_temporal_metrics(original_agg, dp_agg, NUMERIC_COLS)

if temporal_metrics:
    print(f"{'Column':<25} {'Daily Corr':>12} {'Trend Pres':>12} {'DoW Corr':>12} {'Peak Acc':>12}")
    print("-"*75)
    
    for col in NUMERIC_COLS:
        if col not in temporal_metrics:
            continue
        r = temporal_metrics[col]
        daily = r.get('daily_correlation', np.nan)
        trend = "‚úÖ" if r.get('trend_preserved', False) else "‚ùå"
        dow = r.get('dow_correlation', np.nan)
        peak = r.get('peak_detection_accuracy', np.nan) * 100
        
        daily_status = "‚úÖ" if daily > 0.9 else ("‚ö†Ô∏è" if daily > 0.7 else "‚ùå")
        print(f"{col:<25} {daily:>11.4f}{daily_status} {trend:>12} {dow:>12.4f} {peak:>11.0f}%")
    
    # Score
    temporal_scores = []
    for col in NUMERIC_COLS:
        if col in temporal_metrics:
            r = temporal_metrics[col]
            daily = r.get('daily_correlation', 0.5)
            peak = r.get('peak_detection_accuracy', 0.5)
            temporal_scores.append((daily * 100 + peak * 100) / 2)
    
    temporal_score = np.mean(temporal_scores) if temporal_scores else 50.0
else:
    temporal_score = 50.0
    print("Insufficient temporal data for analysis")

print(f"\\n{'='*70}")
print(f"TEMPORAL PRESERVATION SCORE: {temporal_score:.1f}/100")
'''

# =============================================================================
# CELL: Code - B4. Geographic Consistency
# =============================================================================
CELL_B4_GEOGRAPHIC = '''
print("\\n" + "="*80)
print("B4. GEOGRAPHIC CONSISTENCY")
print("="*80)
print("\\nMeasuring hierarchical consistency (Province ‚Üí City).\\n")

def compute_geographic_consistency(orig, synth, columns, city_col='city', province_col='province'):
    """
    Compute geographic hierarchy consistency metrics.
    
    Metrics:
    - Province-level accuracy (aggregated from cities)
    - City ranking within province
    - Hierarchical consistency ratio
    """
    results = {}
    
    if province_col not in orig.columns or city_col not in orig.columns:
        return results
    
    for col in columns:
        col_results = {}
        
        # Province-level aggregates
        orig_prov = orig.groupby(province_col)[col].sum()
        synth_prov = synth.groupby(province_col)[col].sum()
        
        common_prov = orig_prov.index.intersection(synth_prov.index)
        if len(common_prov) < 2:
            continue
        
        o_prov = orig_prov.loc[common_prov]
        s_prov = synth_prov.loc[common_prov]
        
        # Province accuracy
        prov_errors = np.abs(s_prov - o_prov)
        prov_rel_errors = prov_errors / (o_prov + 1e-10) * 100
        
        col_results['province_mae'] = prov_errors.mean()
        col_results['province_mape'] = prov_rel_errors.mean()
        
        # Province ranking
        if len(o_prov) > 2:
            prov_rank_corr, _ = spearmanr(o_prov, s_prov)
            col_results['province_rank_correlation'] = prov_rank_corr
        
        # City ranking within each province
        within_prov_corrs = []
        for prov in common_prov:
            orig_cities = orig[orig[province_col] == prov].groupby(city_col)[col].sum()
            synth_cities = synth[synth[province_col] == prov].groupby(city_col)[col].sum()
            
            common_cities = orig_cities.index.intersection(synth_cities.index)
            if len(common_cities) >= 3:
                corr, _ = spearmanr(orig_cities.loc[common_cities], synth_cities.loc[common_cities])
                within_prov_corrs.append(corr)
        
        if within_prov_corrs:
            col_results['within_province_rank_corr'] = np.mean(within_prov_corrs)
        
        results[col] = col_results
    
    return results

geo_consistency = compute_geographic_consistency(original_agg, dp_agg, NUMERIC_COLS)

if geo_consistency:
    print(f"{'Column':<25} {'Prov MAPE':>12} {'Prov Rank':>12} {'City Rank':>12}")
    print("-"*65)
    
    for col in NUMERIC_COLS:
        if col not in geo_consistency:
            continue
        r = geo_consistency[col]
        mape = r.get('province_mape', np.nan)
        prov_rank = r.get('province_rank_correlation', np.nan)
        city_rank = r.get('within_province_rank_corr', np.nan)
        
        mape_status = "‚úÖ" if mape < 10 else ("‚ö†Ô∏è" if mape < 25 else "‚ùå")
        print(f"{col:<25} {mape:>11.1f}%{mape_status} {prov_rank:>12.4f} {city_rank:>12.4f}")
    
    # Score
    geo_scores = []
    for col in NUMERIC_COLS:
        if col in geo_consistency:
            r = geo_consistency[col]
            mape_score = max(0, 100 - r.get('province_mape', 50))
            rank_score = r.get('province_rank_correlation', 0.5) * 100
            geo_scores.append((mape_score + rank_score) / 2)
    
    geographic_score = np.mean(geo_scores) if geo_scores else 50.0
else:
    geographic_score = 50.0
    print("Insufficient geographic data for analysis")

print(f"\\n{'='*70}")
print(f"GEOGRAPHIC CONSISTENCY SCORE: {geographic_score:.1f}/100")
'''

# =============================================================================
# CELL: Code - Final Summary
# =============================================================================
CELL_FINAL_SUMMARY = '''
print("\\n" + "="*80)
print("COMPREHENSIVE EVALUATION SUMMARY")
print("="*80)

# Collect all scores
all_scores = {
    'Statistical Fidelity': stat_fidelity_score,
    'Distributional Fidelity': dist_fidelity_score,
    'Structural Fidelity': struct_fidelity_score,
    'Per-Cell Error': percell_fidelity_score,
    'Query Accuracy': query_accuracy_score,
    'Ranking Preservation': ranking_score,
    'Temporal Preservation': temporal_score,
    'Geographic Consistency': geographic_score,
}

# Compute weighted overall score
weights = {
    'Statistical Fidelity': 0.10,
    'Distributional Fidelity': 0.10,
    'Structural Fidelity': 0.10,
    'Per-Cell Error': 0.15,
    'Query Accuracy': 0.20,
    'Ranking Preservation': 0.15,
    'Temporal Preservation': 0.10,
    'Geographic Consistency': 0.10,
}

weighted_score = sum(all_scores[k] * weights[k] for k in all_scores)

print(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    DIFFERENTIAL PRIVACY EVALUATION REPORT                    ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  Privacy Budget (œÅ): {float(config.privacy.total_rho):<10.4f}                                        ‚ïë
‚ïë  Total Cells:        {len(original_agg):<10,}                                        ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                              FIDELITY SCORES                                 ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£""")

for name in ['Statistical Fidelity', 'Distributional Fidelity', 'Structural Fidelity', 'Per-Cell Error']:
    score = all_scores[name]
    bar = "‚ñà" * int(score / 5) + "‚ñë" * (20 - int(score / 5))
    status = "‚úÖ" if score >= 80 else ("‚ö†Ô∏è" if score >= 60 else "‚ùå")
    print(f"‚ïë  {name:<25} {bar} {score:>5.1f}/100 {status}  ‚ïë")

print("""‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                              UTILITY SCORES                                  ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£""")

for name in ['Query Accuracy', 'Ranking Preservation', 'Temporal Preservation', 'Geographic Consistency']:
    score = all_scores[name]
    bar = "‚ñà" * int(score / 5) + "‚ñë" * (20 - int(score / 5))
    status = "‚úÖ" if score >= 80 else ("‚ö†Ô∏è" if score >= 60 else "‚ùå")
    print(f"‚ïë  {name:<25} {bar} {score:>5.1f}/100 {status}  ‚ïë")

# Grade
if weighted_score >= 90:
    grade = "A+"
    desc = "Excellent"
elif weighted_score >= 85:
    grade = "A"
    desc = "Very Good"
elif weighted_score >= 80:
    grade = "A-"
    desc = "Good"
elif weighted_score >= 75:
    grade = "B+"
    desc = "Above Average"
elif weighted_score >= 70:
    grade = "B"
    desc = "Average"
elif weighted_score >= 65:
    grade = "B-"
    desc = "Below Average"
elif weighted_score >= 60:
    grade = "C"
    desc = "Acceptable"
else:
    grade = "D"
    desc = "Poor"

print(f"""‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                           OVERALL ASSESSMENT                                 ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                                              ‚ïë
‚ïë         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         ‚ïë
‚ïë         ‚ñà                                                          ‚ñà         ‚ïë
‚ïë         ‚ñà     OVERALL SCORE: {weighted_score:>5.1f}/100    GRADE: {grade:<3}              ‚ñà         ‚ïë
‚ïë         ‚ñà     Assessment: {desc:<15}                         ‚ñà         ‚ïë
‚ïë         ‚ñà                                                          ‚ñà         ‚ïë
‚ïë         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         ‚ïë
‚ïë                                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")

# Recommendations
print("\\nüìã RECOMMENDATIONS:")
print("-" * 70)

weak_areas = [k for k, v in all_scores.items() if v < 70]
if weak_areas:
    for area in weak_areas:
        print(f"  ‚ö†Ô∏è {area} score is low ({all_scores[area]:.1f})")
        if 'Fidelity' in area:
            print("     ‚Üí Consider increasing privacy budget (œÅ) for better accuracy")
        elif 'Ranking' in area:
            print("     ‚Üí Rankings are sensitive to noise; consider larger aggregations")
        elif 'Query' in area:
            print("     ‚Üí Query accuracy can be improved with higher privacy budget")
        elif 'Temporal' in area:
            print("     ‚Üí Time patterns may need dedicated DP allocation")
        elif 'Geographic' in area:
            print("     ‚Üí Consider post-processing for hierarchical consistency")
else:
    print("  ‚úÖ All metrics are at acceptable levels!")
    print("  ‚úÖ The DP-protected data maintains good utility for analysis.")

print("\\n" + "="*80)
'''

# =============================================================================
# CELL: Code - Export Report
# =============================================================================
CELL_EXPORT = '''
# Export comprehensive report to JSON
report = {
    'metadata': {
        'timestamp': datetime.now().isoformat(),
        'privacy_budget_rho': str(config.privacy.total_rho),
        'total_cells': len(original_agg),
        'columns_evaluated': NUMERIC_COLS,
    },
    'scores': {k: float(v) for k, v in all_scores.items()},
    'overall_score': float(weighted_score),
    'grade': grade,
    'statistical_fidelity': {
        col: {
            'mean_moment_error': r['mean_moment_error'],
            'mean_quantile_error': r['mean_quantile_error'],
        } for col, r in stat_fidelity.items()
    },
    'distributional_fidelity': {
        col: {
            'ks_statistic': r['ks_statistic'],
            'ks_pvalue': r['ks_pvalue'],
            'js_similarity': r['js_similarity'],
            'tv_similarity': r['tv_similarity'],
        } for col, r in dist_fidelity.items()
    },
    'query_accuracy': {
        col: {
            'point_rmse': r['point_rmse'],
            'point_mape': r['point_mape'],
            'total_error_pct': r['total_error_pct'],
        } for col, r in query_accuracy.items()
    },
}

# Save report
report_path = 'output/dp_evaluation_report.json'
os.makedirs(os.path.dirname(report_path), exist_ok=True)

with open(report_path, 'w') as f:
    json.dump(report, f, indent=2, default=str)

print(f"‚úÖ Comprehensive evaluation report saved to: {report_path}")
print(f"\\nReport sections: {list(report.keys())}")
'''

# =============================================================================
# Print all cells for reference
# =============================================================================
if __name__ == "__main__":
    cells = [
        ("markdown", CELL_HEADER),
        ("code", CELL_SETUP),
        ("code", CELL_LOAD_DATA),
        ("markdown", CELL_PART_A_HEADER),
        ("code", CELL_A1_STATISTICAL),
        ("code", CELL_A2_DISTRIBUTIONAL),
        ("code", CELL_A3_STRUCTURAL),
        ("code", CELL_A4_PERCELL),
        ("code", CELL_A5_STRATIFIED),
        ("markdown", CELL_PART_B_HEADER),
        ("code", CELL_B1_QUERY),
        ("code", CELL_B2_RANKING),
        ("code", CELL_B3_TEMPORAL),
        ("code", CELL_B4_GEOGRAPHIC),
        ("code", CELL_FINAL_SUMMARY),
        ("code", CELL_EXPORT),
    ]
    
    print("Generated", len(cells), "cells for notebook")
    for i, (cell_type, content) in enumerate(cells):
        print(f"  Cell {i}: {cell_type} ({len(content)} chars)")

