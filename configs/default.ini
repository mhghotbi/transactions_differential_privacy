# Transaction DP System - Default Configuration
# =============================================
# Copy this file and modify for your use case

[privacy]
# Total privacy budget (rho for zCDP)
# Can be specified as fraction (e.g., 1/4) or decimal
# Monthly release: rho=0.25 → Annual rho=3 → epsilon≈20
total_rho = 1/4

# Delta parameter for (epsilon, delta)-DP conversion
delta = 1e-10

# Geographic budget allocation (must sum to 1.0)
geographic_split_province = 0.2
geographic_split_city = 0.8

# Query budget allocation (must sum to 1.0)
query_split_transaction_count = 0.25
query_split_unique_cards = 0.25
query_split_unique_acceptors = 0.25
query_split_total_amount = 0.25

# Bounded Contribution Settings
# =============================
# Limits how many transactions each card can contribute per cell (city, mcc, day)
# This bounds the sensitivity for transaction_count query

# Method options (see detailed explanations below):
# - 'transaction_weighted_percentile': Keep 99% of TRANSACTIONS (RECOMMENDED - minimal data loss)
# - 'percentile': Keep 99% of CELLS intact (can lose 50%+ transactions if distribution is skewed)
# - 'iqr': Statistical outlier detection (K = Q3 + 1.5*IQR)
# - 'fixed': Use a fixed K value
#
# RECOMMENDED: Use 'transaction_weighted_percentile' to minimize data loss
# This finds K such that (transactions_kept / total_transactions) >= percentile/100
# Example: percentile=99 means keep 99% of transactions (~1% data loss)
#
# The old 'percentile' method finds K = p-th percentile of cell counts
# This keeps p% of cells intact, but can discard 50%+ of transactions if distribution is skewed
# Example: If 1% of cells contain 60% of transactions, those get clipped heavily
contribution_bound_method = transaction_weighted_percentile

# For 'iqr' method: multiplier for IQR (default 1.5 for standard outlier detection)
# K = ceil(Q3 + multiplier * IQR) where IQR = Q3 - Q1
contribution_bound_iqr_multiplier = 1.5

# For 'fixed' method: use this fixed value
contribution_bound_fixed = 5

# For 'percentile' method: use this percentile (e.g., 99)
# 99th percentile means 99% of card-cell combinations have <= K transactions
contribution_bound_percentile = 99

# Compute K per MCC group for memory efficiency (recommended for large datasets)
# Set to false to compute K globally across all MCCs (uses more memory)
contribution_bound_per_group = true

# Suppression Settings
# ====================
# Suppress cells with noisy count below threshold (Census 2020 style)
# Threshold: minimum noisy count to release (0 = no suppression)
suppression_threshold = 5

# Suppression method: 'flag' (add column), 'null' (set to NULL), 'value' (sentinel)
suppression_method = flag

# Sentinel value for suppressed cells (only used if method = value)
suppression_sentinel = -1

# Confidence Interval Settings
# ============================
# Output confidence intervals with protected values
# Comma-separated list of levels (e.g., 0.90,0.95 for 90% and 95% CI)
confidence_levels = 0.95

# Include relative margin of error (MOE as percentage of value)
include_relative_moe = true

# Global Sensitivity Settings
# ===========================
# How to compute L2 sensitivity for cards appearing in multiple cells
# 'local': assume each card in 1 cell (sensitivity = K)
# 'global': compute max cells per card from data (sensitivity = sqrt(M) * K)
# 'fixed': use fixed max cells value (sensitivity = sqrt(fixed_max_cells) * K)
sensitivity_method = global

# For 'fixed' method: maximum cells a card can appear in
fixed_max_cells_per_card = 100

# MCC Grouping Settings
# ====================
# Group MCCs by transaction amount magnitude for stratified sensitivity
# TWO separate groupings are used:
# 1. K computation grouping (mcc_num_groups_for_k): Smaller, for fast K calculation
# 2. DP noise grouping (mcc_num_groups): Larger, for better privacy accounting
mcc_grouping_enabled = true

# Number of MCC groups for K computation (smaller = faster, conservative bound)
# Example: 5 groups → compute K₁...K₅, then take MAX
mcc_num_groups_for_k = 5

# Number of MCC groups for DP noise application (parallel composition)
# Example: 10 groups → apply noise separately to each group
mcc_num_groups = 10

# Percentile for computing per-group winsorization cap
mcc_group_cap_percentile = 99.0

[data]
# Input data path (Parquet or CSV)
input_path = data/transactions.parquet

# Output path for protected data
output_path = output/protected/

# Path to city-province mapping CSV
city_province_path = data/city_province.csv

# Input format: parquet or csv
input_format = parquet

# Winsorization settings
# Percentile for automatic cap computation (0-100)
winsorize_percentile = 99.0
# Or set a fixed cap (uncomment to use):
# winsorize_cap = 100000000

# Date settings
date_column = transaction_date
date_format = %%Y-%%m-%%d
num_days = 30

[spark]
# Spark application name
app_name = TransactionDP

# Spark master URL
# local[*] for local mode, spark://host:7077 for cluster
master = local[*]

# Memory settings
executor_memory = 4g
driver_memory = 2g

# Shuffle partitions
shuffle_partitions = 200

[columns]
# Column name mappings (source_column = standard_name)
# Modify if your data has different column names
transaction_id = transaction_id
amount = amount
transaction_date = transaction_date
card_number = card_number
acceptor_id = acceptor_id
acceptor_city = acceptor_city
mcc = mcc

