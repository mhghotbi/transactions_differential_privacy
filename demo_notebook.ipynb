{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transaction DP System - Production Test Notebook\n",
        "\n",
        "This notebook provides comprehensive testing of the differential privacy pipeline for transaction data.\n",
        "\n",
        "**Test Coverage:**\n",
        "1. ‚úÖ Data generation and loading\n",
        "2. ‚úÖ Privacy configuration and budget allocation\n",
        "3. ‚úÖ User-level DP parameters (D_max, K, sensitivities)\n",
        "4. ‚úÖ Pipeline execution with top-down algorithm\n",
        "5. ‚úÖ Privacy guarantee verification\n",
        "6. ‚úÖ Utility evaluation metrics\n",
        "\n",
        "**Key Privacy Concepts:**\n",
        "- **zCDP (œÅ-zCDP)**: Privacy budget measured in rho, converts to (Œµ,Œ¥)-DP\n",
        "- **User-level DP**: Protects entire card's transaction history (not just single transactions)\n",
        "- **Global Sensitivity**: sqrt(M √ó D_max) √ó K where M=max cells per card, D_max=max distinct days\n",
        "- **Sequential Composition**: Budget accumulates across days within a month\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup & Environment Configuration\n",
        "\n",
        "Configure logging, imports, and verify environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import logging\n",
        "import math\n",
        "from datetime import datetime\n",
        "from fractions import Fraction\n",
        "\n",
        "# Configure logging to print to stdout (Jupyter/terminal)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n",
        "    datefmt='%H:%M:%S',\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    force=True  # Override any existing config\n",
        ")\n",
        "\n",
        "# Set log level for all transaction_dp loggers\n",
        "logging.getLogger('transaction_dp').setLevel(logging.INFO)\n",
        "logging.getLogger('py4j').setLevel(logging.WARNING)  # Reduce Spark noise\n",
        "\n",
        "logger = logging.getLogger('demo_notebook')\n",
        "\n",
        "# Print environment info\n",
        "print(\"=\"*70)\n",
        "print(\"ENVIRONMENT INFORMATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Python Version: {sys.version}\")\n",
        "print(f\"Working Directory: {os.getcwd()}\")\n",
        "print(f\"Timestamp: {datetime.now().isoformat()}\")\n",
        "\n",
        "# Check required files exist\n",
        "required_files = [\n",
        "    'data/city_province.csv',\n",
        "    'core/config.py',\n",
        "    'core/pipeline.py',\n",
        "    'core/sensitivity.py',\n",
        "    'engine/topdown.py'\n",
        "]\n",
        "print(f\"\\nRequired Files Check:\")\n",
        "for f in required_files:\n",
        "    exists = os.path.exists(f)\n",
        "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
        "    print(f\"  {status} {f}\")\n",
        "    if not exists:\n",
        "        raise FileNotFoundError(f\"Required file missing: {f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Environment setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Spark Configuration & Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Spark configuration - adjust based on your machine\n",
        "SPARK_MASTER = \"local[*]\"  # Use all available cores\n",
        "SPARK_APP_NAME = \"TransactionDP-Test\"\n",
        "SPARK_EXECUTOR_MEMORY = \"8g\"  # Adjust based on available RAM\n",
        "SPARK_DRIVER_MEMORY = \"8g\"\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SPARK CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Master: {SPARK_MASTER}\")\n",
        "print(f\"  App Name: {SPARK_APP_NAME}\")\n",
        "print(f\"  Executor Memory: {SPARK_EXECUTOR_MEMORY}\")\n",
        "print(f\"  Driver Memory: {SPARK_DRIVER_MEMORY}\")\n",
        "\n",
        "# Stop any existing Spark session\n",
        "existing_session = SparkSession.getActiveSession()\n",
        "if existing_session:\n",
        "    print(\"\\nStopping existing Spark session...\")\n",
        "    existing_session.stop()\n",
        "    import time\n",
        "    time.sleep(0.5)\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(SPARK_APP_NAME) \\\n",
        "    .master(SPARK_MASTER) \\\n",
        "    .config(\"spark.executor.memory\", SPARK_EXECUTOR_MEMORY) \\\n",
        "    .config(\"spark.driver.memory\", SPARK_DRIVER_MEMORY) \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Verify Spark session\n",
        "actual_master = spark.sparkContext.master\n",
        "actual_parallelism = spark.sparkContext.defaultParallelism\n",
        "\n",
        "print(f\"\\n‚úÖ Spark session initialized!\")\n",
        "print(f\"  Actual Master: {actual_master}\")\n",
        "print(f\"  Default Parallelism: {actual_parallelism}\")\n",
        "print(f\"  Spark Version: {spark.version}\")\n",
        "\n",
        "# Helper functions\n",
        "def show_df(df, n=20, truncate=True):\n",
        "    \"\"\"Display Spark DataFrame in notebook.\"\"\"\n",
        "    df.show(n=n, truncate=truncate)\n",
        "    \n",
        "def to_pandas_safe(df, max_rows=100000):\n",
        "    \"\"\"Convert Spark DataFrame to Pandas, but only if small enough.\"\"\"\n",
        "    count = df.count()\n",
        "    if count > max_rows:\n",
        "        raise ValueError(f\"DataFrame too large ({count:,} rows). Use Spark operations.\")\n",
        "    return df.toPandas()\n",
        "\n",
        "print(\"\\nüìù Helper functions available: show_df(), to_pandas_safe()\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Generate Test Data\n",
        "\n",
        "Generate synthetic transaction data for testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from examples.generate_sample_data import generate_sample_data\n",
        "\n",
        "# Test data configuration\n",
        "# For production testing, increase NUM_RECORDS to 100K-1M\n",
        "NUM_RECORDS = 50000       # Number of transactions\n",
        "NUM_DAYS = 30             # Time span in days (1 month)\n",
        "NUM_CARDS = 2000          # Number of unique cards\n",
        "NUM_ACCEPTORS = 500       # Number of unique merchants\n",
        "SEED = 42                 # Random seed for reproducibility\n",
        "\n",
        "# Paths\n",
        "CITY_PROVINCE_PATH = 'data/city_province.csv'\n",
        "DATA_OUTPUT_PATH = 'data/demo_transactions.parquet'\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATA GENERATION CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Records: {NUM_RECORDS:,}\")\n",
        "print(f\"  Days: {NUM_DAYS}\")\n",
        "print(f\"  Unique Cards: {NUM_CARDS:,}\")\n",
        "print(f\"  Unique Acceptors: {NUM_ACCEPTORS:,}\")\n",
        "print(f\"  Random Seed: {SEED}\")\n",
        "print(f\"  Output: {DATA_OUTPUT_PATH}\")\n",
        "\n",
        "# Generate data\n",
        "print(f\"\\nGenerating {NUM_RECORDS:,} transactions...\")\n",
        "generate_sample_data(\n",
        "    num_records=NUM_RECORDS,\n",
        "    output_path=DATA_OUTPUT_PATH,\n",
        "    city_province_path=CITY_PROVINCE_PATH,\n",
        "    num_days=NUM_DAYS,\n",
        "    num_cards=NUM_CARDS,\n",
        "    num_acceptors=NUM_ACCEPTORS,\n",
        "    seed=SEED,\n",
        "    spark_master=SPARK_MASTER,\n",
        "    output_format='parquet'\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Data generated: {DATA_OUTPUT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Load and Analyze Raw Data\n",
        "\n",
        "Understand data characteristics for privacy parameter tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "df_spark = spark.read.parquet(DATA_OUTPUT_PATH)\n",
        "\n",
        "# Basic statistics\n",
        "total_count = df_spark.count()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"RAW DATA ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal records: {total_count:,}\")\n",
        "print(f\"\\nSchema:\")\n",
        "df_spark.printSchema()\n",
        "\n",
        "# Unique counts\n",
        "unique_cards = df_spark.select('card_number').distinct().count()\n",
        "unique_acceptors = df_spark.select('acceptor_id').distinct().count()\n",
        "unique_cities = df_spark.select('acceptor_city').distinct().count()\n",
        "unique_mccs = df_spark.select('mcc').distinct().count()\n",
        "\n",
        "print(f\"\\nüìä Unique Counts:\")\n",
        "print(f\"  Cards: {unique_cards:,}\")\n",
        "print(f\"  Acceptors: {unique_acceptors:,}\")\n",
        "print(f\"  Cities: {unique_cities:,}\")\n",
        "print(f\"  MCCs: {unique_mccs:,}\")\n",
        "\n",
        "# Date and amount ranges\n",
        "stats = df_spark.agg(\n",
        "    F.min('transaction_date').alias('min_date'),\n",
        "    F.max('transaction_date').alias('max_date'),\n",
        "    F.min('amount').alias('min_amount'),\n",
        "    F.max('amount').alias('max_amount'),\n",
        "    F.avg('amount').alias('avg_amount'),\n",
        "    F.stddev('amount').alias('std_amount'),\n",
        "    F.percentile_approx('amount', 0.99).alias('p99_amount')\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"\\nüìÖ Date Range: {stats['min_date']} to {stats['max_date']}\")\n",
        "print(f\"\\nüí∞ Amount Statistics:\")\n",
        "print(f\"  Min: {stats['min_amount']:,.0f}\")\n",
        "print(f\"  Max: {stats['max_amount']:,.0f}\")\n",
        "print(f\"  Mean: {stats['avg_amount']:,.0f}\")\n",
        "print(f\"  Std Dev: {stats['std_amount']:,.0f}\")\n",
        "print(f\"  99th Percentile: {stats['p99_amount']:,.0f}\")\n",
        "\n",
        "# Sample data\n",
        "print(f\"\\nüìù Sample rows:\")\n",
        "show_df(df_spark, n=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 User-Level DP Parameters Analysis\n",
        "\n",
        "Compute critical parameters for user-level differential privacy:\n",
        "- **M**: Max cells (city√óMCC√óday combinations) a single card appears in\n",
        "- **D_max**: Max distinct days a single card makes transactions\n",
        "- **K**: Per-cell contribution bound\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"USER-LEVEL DP PARAMETER ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Compute M: Max cells per card\n",
        "# A cell is (city, mcc, day) combination\n",
        "cells_per_card = df_spark.groupBy('card_number', 'acceptor_city', 'mcc', 'transaction_date') \\\n",
        "    .count() \\\n",
        "    .groupBy('card_number') \\\n",
        "    .agg(F.count('*').alias('num_cells'))\n",
        "\n",
        "M_stats = cells_per_card.agg(\n",
        "    F.max('num_cells').alias('max_M'),\n",
        "    F.avg('num_cells').alias('avg_M'),\n",
        "    F.percentile_approx('num_cells', 0.99).alias('p99_M'),\n",
        "    F.percentile_approx('num_cells', 0.95).alias('p95_M')\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"\\nüìä M (Max Cells per Card):\")\n",
        "print(f\"  Max: {M_stats['max_M']}\")\n",
        "print(f\"  99th Percentile: {M_stats['p99_M']}\")\n",
        "print(f\"  95th Percentile: {M_stats['p95_M']}\")\n",
        "print(f\"  Mean: {M_stats['avg_M']:.2f}\")\n",
        "\n",
        "# Compute D_max: Max distinct days per card\n",
        "days_per_card = df_spark.groupBy('card_number') \\\n",
        "    .agg(F.countDistinct('transaction_date').alias('num_days'))\n",
        "\n",
        "D_stats = days_per_card.agg(\n",
        "    F.max('num_days').alias('max_D'),\n",
        "    F.avg('num_days').alias('avg_D'),\n",
        "    F.percentile_approx('num_days', 0.99).alias('p99_D')\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"\\nüìÖ D_max (Max Distinct Days per Card):\")\n",
        "print(f\"  Max: {D_stats['max_D']}\")\n",
        "print(f\"  99th Percentile: {D_stats['p99_D']}\")\n",
        "print(f\"  Mean: {D_stats['avg_D']:.2f}\")\n",
        "\n",
        "# Compute K: Transactions per cell\n",
        "txns_per_cell = df_spark.groupBy('card_number', 'acceptor_city', 'mcc', 'transaction_date') \\\n",
        "    .agg(F.count('*').alias('txns_in_cell'))\n",
        "\n",
        "K_stats = txns_per_cell.agg(\n",
        "    F.max('txns_in_cell').alias('max_K'),\n",
        "    F.avg('txns_in_cell').alias('avg_K'),\n",
        "    F.percentile_approx('txns_in_cell', 0.99).alias('p99_K'),\n",
        "    F.percentile_approx('txns_in_cell', 0.75).alias('p75_K')\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"\\nüî¢ K (Transactions per Card per Cell):\")\n",
        "print(f\"  Max: {K_stats['max_K']}\")\n",
        "print(f\"  99th Percentile: {K_stats['p99_K']}\")\n",
        "print(f\"  75th Percentile: {K_stats['p75_K']}\")\n",
        "print(f\"  Mean: {K_stats['avg_K']:.2f}\")\n",
        "\n",
        "# Store computed values for later use\n",
        "COMPUTED_M = int(M_stats['max_M'])\n",
        "COMPUTED_D_MAX = int(D_stats['max_D'])\n",
        "COMPUTED_K = int(K_stats['p99_K'])  # Use 99th percentile for bounded contribution\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"COMPUTED PARAMETERS FOR DP:\")\n",
        "print(f\"  M (max cells per card): {COMPUTED_M}\")\n",
        "print(f\"  D_max (max days per card): {COMPUTED_D_MAX}\")\n",
        "print(f\"  K (contribution bound): {COMPUTED_K}\")\n",
        "print(f\"  sqrt(M √ó D_max) √ó K = {math.sqrt(COMPUTED_M * COMPUTED_D_MAX) * COMPUTED_K:.2f}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Configure DP Pipeline\n",
        "\n",
        "Set up differential privacy configuration with all parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from core.config import Config\n",
        "\n",
        "# Create configuration\n",
        "config = Config()\n",
        "\n",
        "# === DATA SETTINGS ===\n",
        "config.data.input_path = DATA_OUTPUT_PATH\n",
        "config.data.output_path = 'output/demo_dp_results'\n",
        "config.data.city_province_path = CITY_PROVINCE_PATH\n",
        "config.data.input_format = 'parquet'\n",
        "config.data.num_days = NUM_DAYS\n",
        "config.data.winsorize_percentile = 99.0  # Cap amounts at 99th percentile\n",
        "\n",
        "# === PRIVACY SETTINGS ===\n",
        "# Total privacy budget (rho for zCDP)\n",
        "# Rule of thumb: rho=1 gives strong utility, rho=0.25 gives strong privacy\n",
        "config.privacy.total_rho = Fraction(1, 2)  # rho = 0.5\n",
        "config.privacy.delta = 1e-10\n",
        "\n",
        "# Geographic budget split (Province vs City level)\n",
        "config.privacy.geographic_split = {\n",
        "    'province': 0.2,  # 20% for province-level aggregates\n",
        "    'city': 0.8       # 80% for city-level aggregates\n",
        "}\n",
        "\n",
        "# Query budget split - allocate more to primary queries\n",
        "config.privacy.query_split = {\n",
        "    'transaction_count': 0.20,\n",
        "    'unique_cards': 0.30,       # Higher weight for primary query\n",
        "    'unique_acceptors': 0.30,   # Higher weight for primary query\n",
        "    'total_amount': 0.20\n",
        "}\n",
        "\n",
        "# Bounded contribution settings\n",
        "config.privacy.contribution_bound_method = 'percentile'\n",
        "config.privacy.contribution_bound_percentile = 99.0\n",
        "\n",
        "# Suppression settings\n",
        "config.privacy.suppression_threshold = 5\n",
        "\n",
        "# Sensitivity method\n",
        "config.privacy.sensitivity_method = 'global'\n",
        "\n",
        "# MCC grouping for stratified sensitivity\n",
        "config.privacy.mcc_grouping_enabled = True\n",
        "config.privacy.mcc_num_groups = 5\n",
        "\n",
        "# Confidence intervals\n",
        "config.privacy.confidence_levels = [0.90, 0.95]\n",
        "\n",
        "# === SPARK SETTINGS ===\n",
        "config.spark.app_name = SPARK_APP_NAME\n",
        "config.spark.master = SPARK_MASTER\n",
        "config.spark.executor_memory = SPARK_EXECUTOR_MEMORY\n",
        "config.spark.driver_memory = SPARK_DRIVER_MEMORY\n",
        "\n",
        "# Validate configuration\n",
        "config.validate()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DP CONFIGURATION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüìä Privacy Budget:\")\n",
        "print(f\"  Total œÅ (rho): {config.privacy.total_rho} = {float(config.privacy.total_rho):.4f}\")\n",
        "print(f\"  Œ¥ (delta): {config.privacy.delta}\")\n",
        "\n",
        "# Convert zCDP to (Œµ,Œ¥)-DP for reference\n",
        "rho = float(config.privacy.total_rho)\n",
        "delta = config.privacy.delta\n",
        "epsilon = rho + 2 * math.sqrt(rho * math.log(1/delta))\n",
        "print(f\"  Equivalent (Œµ,Œ¥)-DP: Œµ ‚âà {epsilon:.2f}, Œ¥ = {delta}\")\n",
        "\n",
        "print(f\"\\nüó∫Ô∏è Geographic Budget Split:\")\n",
        "for level, weight in config.privacy.geographic_split.items():\n",
        "    level_rho = rho * weight\n",
        "    print(f\"  {level.capitalize()}: {weight:.0%} ‚Üí œÅ = {level_rho:.4f}\")\n",
        "\n",
        "print(f\"\\nüìã Query Budget Split:\")\n",
        "for query, weight in config.privacy.query_split.items():\n",
        "    query_rho = rho * weight\n",
        "    print(f\"  {query}: {weight:.0%} ‚Üí œÅ = {query_rho:.4f}\")\n",
        "\n",
        "print(f\"\\nüîß Other Settings:\")\n",
        "print(f\"  Contribution Bound Method: {config.privacy.contribution_bound_method}\")\n",
        "print(f\"  Suppression Threshold: {config.privacy.suppression_threshold}\")\n",
        "print(f\"  Sensitivity Method: {config.privacy.sensitivity_method}\")\n",
        "print(f\"  MCC Grouping: {'Enabled' if config.privacy.mcc_grouping_enabled else 'Disabled'}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Configuration validated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Run DP Pipeline\n",
        "\n",
        "Execute the differential privacy pipeline with Top-Down Algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from core.pipeline import DPPipeline\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"EXECUTING DP PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Create and run pipeline\n",
        "pipeline = DPPipeline(config)\n",
        "result = pipeline.run()\n",
        "\n",
        "end_time = datetime.now()\n",
        "duration = (end_time - start_time).total_seconds()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PIPELINE RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if result['success']:\n",
        "    print(f\"\\n‚úÖ SUCCESS!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå FAILED!\")\n",
        "\n",
        "print(f\"\\nüìä Execution Summary:\")\n",
        "print(f\"  Records Processed: {result.get('total_records', 'N/A'):,}\")\n",
        "print(f\"  Privacy Budget Used: œÅ = {result.get('budget_used', 'N/A')}\")\n",
        "print(f\"  Duration: {duration:.2f} seconds\")\n",
        "print(f\"  Output Path: {result.get('output_path', 'N/A')}\")\n",
        "\n",
        "if result.get('errors'):\n",
        "    print(f\"\\n‚ö†Ô∏è Errors:\")\n",
        "    for error in result['errors']:\n",
        "        print(f\"    - {error}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Privacy Verification\n",
        "\n",
        "Verify that privacy guarantees are correctly implemented.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"PRIVACY GUARANTEE VERIFICATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if not result['success']:\n",
        "    print(\"‚ö†Ô∏è Pipeline failed - skipping privacy verification\")\n",
        "else:\n",
        "    import json\n",
        "    \n",
        "    # Load metadata\n",
        "    output_path = config.data.output_path\n",
        "    metadata_path = os.path.join(output_path, 'metadata.json')\n",
        "    \n",
        "    if os.path.exists(metadata_path):\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "        print(f\"\\nüìã Pipeline Metadata:\")\n",
        "        print(json.dumps(metadata, indent=2))\n",
        "    \n",
        "    # Verify budget composition\n",
        "    print(f\"\\nüîê Budget Composition Verification:\")\n",
        "    total_rho = float(config.privacy.total_rho)\n",
        "    print(f\"  Total Budget: œÅ = {total_rho}\")\n",
        "    \n",
        "    # Geographic composition\n",
        "    geo_rho_sum = sum(total_rho * w for w in config.privacy.geographic_split.values())\n",
        "    print(f\"  Geographic Split Sum: {geo_rho_sum:.4f} (should = {total_rho})\")\n",
        "    geo_check = abs(geo_rho_sum - total_rho) < 1e-6\n",
        "    print(f\"  Geographic Composition: {'‚úÖ VALID' if geo_check else '‚ùå INVALID'}\")\n",
        "    \n",
        "    # Query composition\n",
        "    query_sum = sum(config.privacy.query_split.values())\n",
        "    print(f\"  Query Split Sum: {query_sum:.4f} (should = 1.0)\")\n",
        "    query_check = abs(query_sum - 1.0) < 1e-6\n",
        "    print(f\"  Query Composition: {'‚úÖ VALID' if query_check else '‚ùå INVALID'}\")\n",
        "    \n",
        "    # Sensitivity verification\n",
        "    print(f\"\\nüéØ Sensitivity Verification:\")\n",
        "    d_max = config.privacy.computed_d_max or COMPUTED_D_MAX\n",
        "    k_bound = config.privacy.computed_contribution_bound or COMPUTED_K\n",
        "    \n",
        "    print(f\"  D_max (max days): {d_max}\")\n",
        "    print(f\"  K (contribution bound): {k_bound}\")\n",
        "    print(f\"  M (max cells): {COMPUTED_M}\")\n",
        "    \n",
        "    sqrt_md = math.sqrt(COMPUTED_M * d_max)\n",
        "    sens_count = sqrt_md * k_bound\n",
        "    sens_unique = sqrt_md * 1\n",
        "    \n",
        "    print(f\"\\n  Expected Sensitivities (L2):\")\n",
        "    print(f\"    transaction_count: ‚àö(M√óD_max)√óK = {sens_count:.2f}\")\n",
        "    print(f\"    unique_cards: ‚àö(M√óD_max)√ó1 = {sens_unique:.2f}\")\n",
        "    print(f\"    unique_acceptors: ‚àö(M√óD_max)√ó1 = {sens_unique:.2f}\")\n",
        "    \n",
        "    # Privacy guarantee summary\n",
        "    print(f\"\\nüìú PRIVACY GUARANTEE SUMMARY:\")\n",
        "    print(f\"  Mechanism: Discrete Gaussian (zCDP)\")\n",
        "    print(f\"  Privacy Unit: (card_number, month)\")\n",
        "    print(f\"  Composition: Sequential across days, Parallel across cells\")\n",
        "    print(f\"  Total Budget: œÅ = {total_rho} zCDP\")\n",
        "    print(f\"  Equivalent (Œµ,Œ¥)-DP: Œµ ‚âà {epsilon:.2f}, Œ¥ = {delta}\")\n",
        "    \n",
        "    if geo_check and query_check:\n",
        "        print(f\"\\n‚úÖ Privacy verification PASSED!\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Privacy verification FAILED!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. View Results\n",
        "\n",
        "Load and examine the DP-protected output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DP-PROTECTED OUTPUT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "output_path = config.data.output_path\n",
        "\n",
        "if os.path.exists(output_path):\n",
        "    print(f\"\\nüìÅ Output directory: {output_path}\")\n",
        "    print(f\"\\nContents:\")\n",
        "    for item in os.listdir(output_path):\n",
        "        item_path = os.path.join(output_path, item)\n",
        "        if os.path.isfile(item_path):\n",
        "            size = os.path.getsize(item_path)\n",
        "            print(f\"  - {item} ({size:,} bytes)\")\n",
        "        else:\n",
        "            print(f\"  - {item}/\")\n",
        "    \n",
        "    # Load metadata\n",
        "    metadata_path = os.path.join(output_path, 'metadata.json')\n",
        "    if os.path.exists(metadata_path):\n",
        "        print(\"\\nüìã Metadata:\")\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "        print(json.dumps(metadata, indent=2))\n",
        "    \n",
        "    # Load protected data\n",
        "    protected_data_path = os.path.join(output_path, \"protected_data\")\n",
        "    if os.path.exists(protected_data_path):\n",
        "        print(f\"\\nüìä Loading protected data...\")\n",
        "        dp_df = spark.read.parquet(protected_data_path)\n",
        "        dp_count = dp_df.count()\n",
        "        print(f\"  Protected cells: {dp_count:,}\")\n",
        "        print(f\"\\n  Sample:\")\n",
        "        show_df(dp_df, n=10)\n",
        "else:\n",
        "    print(f\"‚ùå Output directory not found: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Utility Evaluation\n",
        "\n",
        "Compare original vs DP-protected data to measure utility loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "from pyspark.sql.functions import col, count, countDistinct, sum as spark_sum\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"UTILITY EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if not result['success']:\n",
        "    print(\"‚ö†Ô∏è Pipeline failed - skipping utility evaluation\")\n",
        "else:\n",
        "    # Aggregate original data to same granularity\n",
        "    print(\"\\nüìä Aggregating original data...\")\n",
        "    original_agg = df_spark.groupBy('acceptor_city', 'mcc', 'transaction_date').agg(\n",
        "        count('transaction_id').alias('transaction_count'),\n",
        "        countDistinct('card_number').alias('unique_cards'),\n",
        "        countDistinct('acceptor_id').alias('unique_acceptors'),\n",
        "        spark_sum('amount').alias('total_amount')\n",
        "    )\n",
        "    \n",
        "    orig_count = original_agg.count()\n",
        "    print(f\"  Original cells: {orig_count:,}\")\n",
        "    \n",
        "    # Load DP data\n",
        "    protected_data_path = os.path.join(output_path, \"protected_data\")\n",
        "    dp_agg = spark.read.parquet(protected_data_path)\n",
        "    dp_count = dp_agg.count()\n",
        "    print(f\"  DP-protected cells: {dp_count:,}\")\n",
        "    \n",
        "    # Compare totals using Spark\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"AGGREGATE LEVEL COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    NUMERIC_COLS = ['transaction_count', 'unique_cards', 'unique_acceptors', 'total_amount']\n",
        "    \n",
        "    for col_name in NUMERIC_COLS:\n",
        "        orig_total = original_agg.agg(spark_sum(col_name)).collect()[0][0] or 0\n",
        "        dp_total = dp_agg.agg(spark_sum(col_name)).collect()[0][0] or 0\n",
        "        \n",
        "        if orig_total > 0:\n",
        "            error_pct = abs(dp_total - orig_total) / orig_total * 100\n",
        "            status = \"‚úÖ\" if error_pct < 5 else (\"‚ö†Ô∏è\" if error_pct < 15 else \"‚ùå\")\n",
        "        else:\n",
        "            error_pct = 0\n",
        "            status = \"‚ö†Ô∏è\"\n",
        "        \n",
        "        print(f\"\\n{col_name}:\")\n",
        "        print(f\"  Original Total: {orig_total:,.0f}\")\n",
        "        print(f\"  DP Total: {dp_total:,.0f}\")\n",
        "        print(f\"  Error: {error_pct:.2f}% {status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 10. Production Readiness Checklist\n",
        "\n",
        "Verify the system is ready for production deployment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"PRODUCTION READINESS CHECKLIST\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "checks = []\n",
        "\n",
        "# 1. Pipeline Success\n",
        "check_1 = result['success']\n",
        "checks.append(('Pipeline Execution', check_1))\n",
        "print(f\"\\n{'‚úÖ' if check_1 else '‚ùå'} Pipeline Execution: {'PASSED' if check_1 else 'FAILED'}\")\n",
        "\n",
        "# 2. Output Files Exist\n",
        "output_exists = os.path.exists(os.path.join(output_path, 'protected_data'))\n",
        "checks.append(('Output Files', output_exists))\n",
        "print(f\"{'‚úÖ' if output_exists else '‚ùå'} Output Files: {'EXIST' if output_exists else 'MISSING'}\")\n",
        "\n",
        "# 3. Metadata Present\n",
        "metadata_exists = os.path.exists(os.path.join(output_path, 'metadata.json'))\n",
        "checks.append(('Metadata', metadata_exists))\n",
        "print(f\"{'‚úÖ' if metadata_exists else '‚ùå'} Metadata: {'PRESENT' if metadata_exists else 'MISSING'}\")\n",
        "\n",
        "# 4. Budget Composition Valid\n",
        "budget_valid = abs(sum(config.privacy.geographic_split.values()) - 1.0) < 1e-6\n",
        "budget_valid = budget_valid and abs(sum(config.privacy.query_split.values()) - 1.0) < 1e-6\n",
        "checks.append(('Budget Composition', budget_valid))\n",
        "print(f\"{'‚úÖ' if budget_valid else '‚ùå'} Budget Composition: {'VALID' if budget_valid else 'INVALID'}\")\n",
        "\n",
        "# 5. No Negative Counts (sanity check)\n",
        "if output_exists:\n",
        "    dp_df = spark.read.parquet(os.path.join(output_path, 'protected_data'))\n",
        "    neg_counts = dp_df.filter(F.col('transaction_count') < 0).count()\n",
        "    no_negative = neg_counts == 0\n",
        "    checks.append(('No Negative Counts', no_negative))\n",
        "    print(f\"{'‚úÖ' if no_negative else '‚ö†Ô∏è'} No Negative Counts: {'PASSED' if no_negative else f'{neg_counts} negative values'}\")\n",
        "\n",
        "# 6. Reasonable Processing Time\n",
        "reasonable_time = duration < 300  # 5 minutes for test data\n",
        "checks.append(('Processing Time', reasonable_time))\n",
        "print(f\"{'‚úÖ' if reasonable_time else '‚ö†Ô∏è'} Processing Time: {duration:.1f}s {'(OK)' if reasonable_time else '(SLOW)'}\")\n",
        "\n",
        "# Summary\n",
        "all_passed = all(c[1] for c in checks)\n",
        "passed_count = sum(1 for c in checks if c[1])\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"SUMMARY: {passed_count}/{len(checks)} checks passed\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if all_passed:\n",
        "    print(f\"\\nüéâ PRODUCTION READY!\")\n",
        "    print(f\"   The DP system has passed all checks and is ready for deployment.\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è NOT READY FOR PRODUCTION\")\n",
        "    print(f\"   Please address the failed checks before deployment.\")\n",
        "    failed = [c[0] for c in checks if not c[1]]\n",
        "    print(f\"   Failed: {', '.join(failed)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 11. Cleanup & Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to clean up generated files\n",
        "# import shutil\n",
        "# \n",
        "# if os.path.exists(DATA_OUTPUT_PATH):\n",
        "#     if os.path.isdir(DATA_OUTPUT_PATH):\n",
        "#         shutil.rmtree(DATA_OUTPUT_PATH)\n",
        "#     else:\n",
        "#         os.remove(DATA_OUTPUT_PATH)\n",
        "#     print(f\"Removed: {DATA_OUTPUT_PATH}\")\n",
        "# \n",
        "# if os.path.exists(config.data.output_path):\n",
        "#     shutil.rmtree(config.data.output_path)\n",
        "#     print(f\"Removed: {config.data.output_path}\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"NOTEBOOK COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTimestamp: {datetime.now().isoformat()}\")\n",
        "print(f\"\\nüìã Summary:\")\n",
        "print(f\"  - Records processed: {result.get('total_records', 'N/A'):,}\")\n",
        "print(f\"  - Privacy budget: œÅ = {config.privacy.total_rho}\")\n",
        "print(f\"  - Pipeline status: {'‚úÖ SUCCESS' if result['success'] else '‚ùå FAILED'}\")\n",
        "print(f\"  - Production ready: {'‚úÖ YES' if all_passed else '‚ùå NO'}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
