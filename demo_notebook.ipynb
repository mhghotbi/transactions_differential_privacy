{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transaction DP System - Production Test Notebook\n",
        "\n",
        "This notebook provides comprehensive testing of the differential privacy pipeline for transaction data.\n",
        "\n",
        "**Test Coverage:**\n",
        "1. ‚úÖ Data loading and validation\n",
        "2. ‚úÖ Privacy configuration and budget allocation\n",
        "3. ‚úÖ User-level DP parameters (D_max, K, sensitivities)\n",
        "4. ‚úÖ Pipeline execution with top-down algorithm\n",
        "5. ‚úÖ Privacy guarantee verification\n",
        "6. ‚úÖ Utility evaluation metrics\n",
        "\n",
        "**Key Privacy Concepts:**\n",
        "- **zCDP (œÅ-zCDP)**: Privacy budget measured in rho, converts to (Œµ,Œ¥)-DP\n",
        "- **User-level DP**: Protects entire card's transaction history (not just single transactions)\n",
        "- **Global Sensitivity**: sqrt(M √ó D_max) √ó K where M=max cells per card, D_max=max distinct days\n",
        "- **Sequential Composition**: Budget accumulates across days within a month\n",
        "\n",
        "**Input Data Schema:**\n",
        "Your data should have these columns:\n",
        "- `pspiin`: PSP identifier (optional)\n",
        "- `acceptorid`: Acceptor/merchant identifier\n",
        "- `card_number`: Card identifier\n",
        "- `transaction_date`: Date of transaction\n",
        "- `transaction_amount`: Transaction amount\n",
        "- `city`: City of the acceptor\n",
        "- `mcc`: Merchant Category Code\n",
        "\n",
        "**Output (with DP):**\n",
        "Aggregated at `(transaction_date, city, mcc)` level with:\n",
        "- `transaction_count`: Count of transactions\n",
        "- `unique_cards`: Count of distinct cards\n",
        "- `transaction_amount_sum`: Sum of transaction amounts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup & Environment Configuration\n",
        "\n",
        "Configure logging, imports, and verify environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import logging\n",
        "import math\n",
        "from datetime import datetime\n",
        "from fractions import Fraction\n",
        "\n",
        "# Add project root to Python path (required for imports to work in Jupyter)\n",
        "PROJECT_ROOT = os.getcwd()\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_ROOT)\n",
        "    print(f\"Added to sys.path: {PROJECT_ROOT}\")\n",
        "\n",
        "# Configure logging to print to stdout (Jupyter/terminal)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n",
        "    datefmt='%H:%M:%S',\n",
        "    handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    force=True  # Override any existing config\n",
        ")\n",
        "\n",
        "# Set log level for all transaction_dp loggers\n",
        "logging.getLogger('transaction_dp').setLevel(logging.INFO)\n",
        "logging.getLogger('py4j').setLevel(logging.WARNING)  # Reduce Spark noise\n",
        "\n",
        "logger = logging.getLogger('demo_notebook')\n",
        "\n",
        "# Print environment info\n",
        "print(\"=\"*70)\n",
        "print(\"ENVIRONMENT INFORMATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Python Version: {sys.version}\")\n",
        "print(f\"Working Directory: {os.getcwd()}\")\n",
        "print(f\"Timestamp: {datetime.now().isoformat()}\")\n",
        "\n",
        "# Check required files exist\n",
        "required_files = [\n",
        "    'data/city_province.csv',\n",
        "    'core/config.py',\n",
        "    'core/pipeline.py',\n",
        "    'core/sensitivity.py',\n",
        "    'engine/topdown.py'\n",
        "]\n",
        "print(f\"\\nRequired Files Check:\")\n",
        "for f in required_files:\n",
        "    exists = os.path.exists(f)\n",
        "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
        "    print(f\"  {status} {f}\")\n",
        "    if not exists:\n",
        "        raise FileNotFoundError(f\"Required file missing: {f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Environment setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Spark Configuration & Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Spark configuration - optimized for 38 cores and 220GB RAM\n",
        "SPARK_MASTER = \"local[38]\"  # Use all 38 available cores\n",
        "SPARK_APP_NAME = \"TransactionDP-Test\"\n",
        "# Memory allocation: 220GB total, leaving ~10GB for OS\n",
        "# - Executor: 170GB (heap memory for processing)\n",
        "# - Driver: 10GB (coordination, not data processing)\n",
        "# - Overhead: 30GB (off-heap, network buffers, etc.)\n",
        "# Total: 170 + 10 + 30 = 210GB, leaving 10GB for OS\n",
        "SPARK_EXECUTOR_MEMORY = \"170g\"\n",
        "SPARK_DRIVER_MEMORY = \"10g\"\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SPARK CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Master: {SPARK_MASTER}\")\n",
        "print(f\"  App Name: {SPARK_APP_NAME}\")\n",
        "print(f\"  Executor Memory: {SPARK_EXECUTOR_MEMORY}\")\n",
        "print(f\"  Driver Memory: {SPARK_DRIVER_MEMORY}\")\n",
        "\n",
        "# Stop any existing Spark session\n",
        "existing_session = SparkSession.getActiveSession()\n",
        "if existing_session:\n",
        "    print(\"\\nStopping existing Spark session...\")\n",
        "    existing_session.stop()\n",
        "    import time\n",
        "    time.sleep(0.5)\n",
        "\n",
        "# Create Spark session with optimizations to reduce RowBasedKeyValueBatch spill warnings\n",
        "# These settings improve memory management during aggregations and joins\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(SPARK_APP_NAME) \\\n",
        "    .master(SPARK_MASTER) \\\n",
        "    .config(\"spark.executor.memory\", SPARK_EXECUTOR_MEMORY) \\\n",
        "    .config(\"spark.driver.memory\", SPARK_DRIVER_MEMORY) \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"228\") \\\n",
        "    .config(\"spark.default.parallelism\", \"228\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"256MB\") \\\n",
        "    .config(\"spark.sql.adaptive.maxNumPostShufflePartitions\", \"500\") \\\n",
        "    .config(\"spark.memory.fraction\", \"0.75\") \\\n",
        "    .config(\"spark.memory.storageFraction\", \"0.4\") \\\n",
        "    .config(\"spark.executor.memoryOverhead\", \"30g\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
        "    .config(\"spark.shuffle.spill.compress\", \"true\") \\\n",
        "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
        "    .config(\"spark.shuffle.spill.numElementsForceSpillThreshold\", \"1000000\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.network.timeout\", \"600s\") \\\n",
        "    .config(\"spark.sql.broadcastTimeout\", \"600s\") \\\n",
        "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"200MB\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Verify Spark session\n",
        "actual_master = spark.sparkContext.master\n",
        "actual_parallelism = spark.sparkContext.defaultParallelism\n",
        "\n",
        "print(f\"\\n‚úÖ Spark session initialized!\")\n",
        "print(f\"  Actual Master: {actual_master}\")\n",
        "print(f\"  Default Parallelism: {actual_parallelism}\")\n",
        "print(f\"  Spark Version: {spark.version}\")\n",
        "\n",
        "# Note about RowBasedKeyValueBatch warnings\n",
        "print(f\"\\nüìù Note: If you see 'RowBasedKeyValueBatch: calling spill()' warnings,\")\n",
        "print(f\"   this is a known Spark behavior during large aggregations.\")\n",
        "print(f\"   The optimizations above help reduce memory pressure, but the warning\")\n",
        "print(f\"   itself is harmless and doesn't affect correctness.\")\n",
        "print(f\"\\nüíª Hardware Configuration:\")\n",
        "print(f\"   - CPU Cores: 38 (fully utilized)\")\n",
        "print(f\"   - Total RAM: 220GB\")\n",
        "print(f\"   - Executor Memory: 170GB (heap)\")\n",
        "print(f\"   - Driver Memory: 10GB\")\n",
        "print(f\"   - Memory Overhead: 30GB (off-heap)\")\n",
        "print(f\"   - Reserved for OS: ~10GB\")\n",
        "print(f\"   - Shuffle Partitions: 228 (6x cores for optimal parallelism)\")\n",
        "print(f\"\\n‚öôÔ∏è Memory Management:\")\n",
        "print(f\"   - Memory Fraction: 75% (balanced heap usage)\")\n",
        "print(f\"   - Storage Fraction: 40% (cache vs execution)\")\n",
        "print(f\"   - Max Result Size: 4GB (prevents driver OOM)\")\n",
        "\n",
        "# Helper functions\n",
        "def show_df(df, n=20, truncate=True):\n",
        "    \"\"\"Display Spark DataFrame in notebook.\"\"\"\n",
        "    df.show(n=n, truncate=truncate)\n",
        "    \n",
        "def to_pandas_safe(df, max_rows=100000):\n",
        "    \"\"\"Convert Spark DataFrame to Pandas, but only if small enough.\"\"\"\n",
        "    count = df.count()\n",
        "    if count > max_rows:\n",
        "        raise ValueError(f\"DataFrame too large ({count:,} rows). Use Spark operations.\")\n",
        "    return df.toPandas()\n",
        "\n",
        "print(\"\\nüìù Helper functions available: show_df(), to_pandas_safe()\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Configure Data Paths\n",
        "\n",
        "**Update the `DATA_INPUT_PATH` below to point to your data file.**\n",
        "\n",
        "Expected columns:\n",
        "- `pspiin`: PSP identifier (optional, not used in DP)\n",
        "- `acceptorid`: Acceptor/merchant identifier  \n",
        "- `card_number`: Card identifier\n",
        "- `transaction_date`: Date of transaction (format: YYYY-MM-DD)\n",
        "- `transaction_amount`: Transaction amount\n",
        "- `city`: City of the acceptor\n",
        "- `mcc`: Merchant Category Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# UPDATE THIS PATH TO YOUR DATA FILE\n",
        "# ============================================\n",
        "DATA_INPUT_PATH = 'data/your_transactions.parquet'  # <-- CHANGE THIS\n",
        "\n",
        "# Other paths\n",
        "CITY_PROVINCE_PATH = 'data/city_province.csv'\n",
        "OUTPUT_PATH = 'data/dp_results'\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATA CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Input Path: {DATA_INPUT_PATH}\")\n",
        "print(f\"  City-Province Mapping: {CITY_PROVINCE_PATH}\")\n",
        "print(f\"  Output Path: {OUTPUT_PATH}\")\n",
        "\n",
        "# Check if input file exists\n",
        "if not os.path.exists(DATA_INPUT_PATH):\n",
        "    print(f\"\\n‚ùå ERROR: Input file not found: {DATA_INPUT_PATH}\")\n",
        "    print(\"   Please update DATA_INPUT_PATH to point to your data file.\")\n",
        "    raise FileNotFoundError(f\"Input file not found: {DATA_INPUT_PATH}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Input file found!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Load and Analyze Raw Data\n",
        "\n",
        "Understand data characteristics for privacy parameter tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "df_spark = spark.read.parquet(DATA_INPUT_PATH)\n",
        "\n",
        "# Basic statistics\n",
        "total_count = df_spark.count()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"RAW DATA ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal records: {total_count:,}\")\n",
        "print(f\"\\nSchema:\")\n",
        "df_spark.printSchema()\n",
        "\n",
        "# Verify required columns exist\n",
        "required_cols = ['card_number', 'transaction_date', 'transaction_amount', 'city', 'mcc']\n",
        "missing_cols = [col for col in required_cols if col not in df_spark.columns]\n",
        "if missing_cols:\n",
        "    print(f\"\\n‚ùå ERROR: Missing required columns: {missing_cols}\")\n",
        "    raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "# Unique counts\n",
        "unique_cards = df_spark.select('card_number').distinct().count()\n",
        "unique_cities = df_spark.select('city').distinct().count()\n",
        "unique_mccs = df_spark.select('mcc').distinct().count()\n",
        "\n",
        "print(f\"\\nüìä Unique Counts:\")\n",
        "print(f\"  Cards: {unique_cards:,}\")\n",
        "print(f\"  Cities: {unique_cities:,}\")\n",
        "print(f\"  MCCs: {unique_mccs:,}\")\n",
        "\n",
        "# Date and amount ranges\n",
        "stats = df_spark.agg(\n",
        "    F.min('transaction_date').alias('min_date'),\n",
        "    F.max('transaction_date').alias('max_date'),\n",
        "    F.min('transaction_amount').alias('min_amount'),\n",
        "    F.max('transaction_amount').alias('max_amount'),\n",
        "    F.avg('transaction_amount').alias('avg_amount'),\n",
        "    F.stddev('transaction_amount').alias('std_amount'),\n",
        "    F.percentile_approx('transaction_amount', 0.99).alias('p99_amount')\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"\\nüìÖ Date Range: {stats['min_date']} to {stats['max_date']}\")\n",
        "print(f\"\\nüí∞ Amount Statistics:\")\n",
        "print(f\"  Min: {stats['min_amount']:,.0f}\")\n",
        "print(f\"  Max: {stats['max_amount']:,.0f}\")\n",
        "print(f\"  Mean: {stats['avg_amount']:,.0f}\")\n",
        "print(f\"  Std Dev: {stats['std_amount']:,.0f}\")\n",
        "print(f\"  99th Percentile: {stats['p99_amount']:,.0f}\")\n",
        "\n",
        "# Sample data\n",
        "print(f\"\\nüìù Sample rows:\")\n",
        "show_df(df_spark, n=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 User-Level DP Parameters Analysis\n",
        "\n",
        "Compute critical parameters for user-level differential privacy:\n",
        "- **M**: Max cells (city√óMCC√óday combinations) a single card appears in\n",
        "- **D_max**: Max distinct days a single card makes transactions\n",
        "- **K**: Per-cell contribution bound\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"USER-LEVEL DP PARAMETER ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Compute M: Max cells per card\n",
        "# A cell is (city, mcc, day) combination\n",
        "cells_per_card = df_spark.groupBy('card_number', 'city', 'mcc', 'transaction_date') \\\n",
        "    .count() \\\n",
        "    .groupBy('card_number') \\\n",
        "    .agg(F.count('*').alias('num_cells'))\n",
        "\n",
        "M_stats = cells_per_card.agg(\n",
        "    F.max('num_cells').alias('max_M'),\n",
        "    F.avg('num_cells').alias('avg_M'),\n",
        "    F.percentile_approx('num_cells', 0.99).alias('p99_M'),\n",
        "    F.percentile_approx('num_cells', 0.95).alias('p95_M')\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"\\nüìä M (Max Cells per Card):\")\n",
        "print(f\"  Max: {M_stats['max_M']}\")\n",
        "print(f\"  99th Percentile: {M_stats['p99_M']}\")\n",
        "print(f\"  95th Percentile: {M_stats['p95_M']}\")\n",
        "print(f\"  Mean: {M_stats['avg_M']:.2f}\")\n",
        "\n",
        "# Compute D_max: Max distinct days per card\n",
        "days_per_card = df_spark.groupBy('card_number') \\\n",
        "    .agg(F.countDistinct('transaction_date').alias('num_days'))\n",
        "\n",
        "D_stats = days_per_card.agg(\n",
        "    F.max('num_days').alias('max_D'),\n",
        "    F.avg('num_days').alias('avg_D'),\n",
        "    F.percentile_approx('num_days', 0.99).alias('p99_D')\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"\\nüìÖ D_max (Max Distinct Days per Card):\")\n",
        "print(f\"  Max: {D_stats['max_D']}\")\n",
        "print(f\"  99th Percentile: {D_stats['p99_D']}\")\n",
        "print(f\"  Mean: {D_stats['avg_D']:.2f}\")\n",
        "\n",
        "# Compute K: Transactions per cell\n",
        "txns_per_cell = df_spark.groupBy('card_number', 'city', 'mcc', 'transaction_date') \\\n",
        "    .agg(F.count('*').alias('txns_in_cell'))\n",
        "\n",
        "K_stats = txns_per_cell.agg(\n",
        "    F.max('txns_in_cell').alias('max_K'),\n",
        "    F.avg('txns_in_cell').alias('avg_K'),\n",
        "    F.percentile_approx('txns_in_cell', 0.99).alias('p99_K'),\n",
        "    F.percentile_approx('txns_in_cell', 0.75).alias('p75_K')\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"\\nüî¢ K (Transactions per Card per Cell):\")\n",
        "print(f\"  Max: {K_stats['max_K']}\")\n",
        "print(f\"  99th Percentile: {K_stats['p99_K']}\")\n",
        "print(f\"  75th Percentile: {K_stats['p75_K']}\")\n",
        "print(f\"  Mean: {K_stats['avg_K']:.2f}\")\n",
        "\n",
        "# Store computed values for later use\n",
        "COMPUTED_M = int(M_stats['max_M'])\n",
        "COMPUTED_D_MAX = int(D_stats['max_D'])\n",
        "COMPUTED_K = int(K_stats['p99_K'])  # Use 99th percentile for bounded contribution\n",
        "\n",
        "# Compute number of days in data\n",
        "min_date = stats['min_date']\n",
        "max_date = stats['max_date']\n",
        "if isinstance(min_date, str):\n",
        "    min_date = datetime.strptime(min_date, '%Y-%m-%d').date()\n",
        "if isinstance(max_date, str):\n",
        "    max_date = datetime.strptime(max_date, '%Y-%m-%d').date()\n",
        "NUM_DAYS = (max_date - min_date).days + 1\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"COMPUTED PARAMETERS FOR DP:\")\n",
        "print(f\"  M (max cells per card): {COMPUTED_M}\")\n",
        "print(f\"  D_max (max days per card): {COMPUTED_D_MAX}\")\n",
        "print(f\"  K (contribution bound): {COMPUTED_K}\")\n",
        "print(f\"  NUM_DAYS (total days in data): {NUM_DAYS}\")\n",
        "print(f\"  sqrt(M √ó D_max) √ó K = {math.sqrt(COMPUTED_M * COMPUTED_D_MAX) * COMPUTED_K:.2f}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Configure DP Pipeline\n",
        "\n",
        "Set up differential privacy configuration with all parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from core.config import Config\n",
        "\n",
        "# Create configuration\n",
        "config = Config()\n",
        "\n",
        "# === DATA SETTINGS ===\n",
        "config.data.input_path = DATA_INPUT_PATH\n",
        "config.data.output_path = OUTPUT_PATH\n",
        "config.data.city_province_path = CITY_PROVINCE_PATH\n",
        "config.data.input_format = 'parquet'\n",
        "config.data.num_days = NUM_DAYS\n",
        "config.data.winsorize_percentile = 99.0  # Cap amounts at 99th percentile\n",
        "\n",
        "# === COLUMN MAPPINGS ===\n",
        "# Map your column names to internal names used by the pipeline\n",
        "config.columns = {\n",
        "    'amount': 'transaction_amount',        # Your amount column\n",
        "    'transaction_date': 'transaction_date', # Your date column\n",
        "    'card_number': 'card_number',          # Your card identifier column\n",
        "    'acceptor_id': 'acceptorid',           # Your acceptor/merchant column\n",
        "    'acceptor_city': 'city',               # Your city column\n",
        "    'mcc': 'mcc'                           # Your MCC column\n",
        "}\n",
        "\n",
        "# === PRIVACY SETTINGS ===\n",
        "# Total privacy budget (rho for zCDP)\n",
        "# Rule of thumb: rho=1 gives strong utility, rho=0.25 gives strong privacy\n",
        "config.privacy.total_rho = Fraction(1, 2)  # rho = 0.5\n",
        "config.privacy.delta = 1e-10\n",
        "\n",
        "# Geographic budget split (Province vs City level)\n",
        "config.privacy.geographic_split = {\n",
        "    'province': 0.2,  # 20% for province-level aggregates\n",
        "    'city': 0.8       # 80% for city-level aggregates\n",
        "}\n",
        "\n",
        "# Query budget split - 3 queries now\n",
        "config.privacy.query_split = {\n",
        "    'transaction_count': 0.34,\n",
        "    'unique_cards': 0.33,\n",
        "    'total_amount': 0.33\n",
        "}\n",
        "\n",
        "# Bounded contribution settings\n",
        "config.privacy.contribution_bound_method = 'percentile'\n",
        "config.privacy.contribution_bound_percentile = 99.0\n",
        "\n",
        "# Suppression settings\n",
        "config.privacy.suppression_threshold = 5\n",
        "\n",
        "# Sensitivity method\n",
        "config.privacy.sensitivity_method = 'global'\n",
        "\n",
        "# MCC grouping for stratified sensitivity\n",
        "config.privacy.mcc_grouping_enabled = True\n",
        "config.privacy.mcc_num_groups = 5\n",
        "\n",
        "# Confidence intervals\n",
        "config.privacy.confidence_levels = [0.90, 0.95]\n",
        "\n",
        "# === SPARK SETTINGS ===\n",
        "config.spark.app_name = SPARK_APP_NAME\n",
        "config.spark.master = SPARK_MASTER\n",
        "config.spark.executor_memory = SPARK_EXECUTOR_MEMORY\n",
        "config.spark.driver_memory = SPARK_DRIVER_MEMORY\n",
        "\n",
        "# Validate configuration\n",
        "config.validate()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DP CONFIGURATION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüìä Privacy Budget:\")\n",
        "print(f\"  Total œÅ (rho): {config.privacy.total_rho} = {float(config.privacy.total_rho):.4f}\")\n",
        "print(f\"  Œ¥ (delta): {config.privacy.delta}\")\n",
        "\n",
        "# Convert zCDP to (Œµ,Œ¥)-DP for reference\n",
        "rho = float(config.privacy.total_rho)\n",
        "delta = config.privacy.delta\n",
        "epsilon = rho + 2 * math.sqrt(rho * math.log(1/delta))\n",
        "print(f\"  Equivalent (Œµ,Œ¥)-DP: Œµ ‚âà {epsilon:.2f}, Œ¥ = {delta}\")\n",
        "\n",
        "print(f\"\\nüó∫Ô∏è Geographic Budget Split:\")\n",
        "for level, weight in config.privacy.geographic_split.items():\n",
        "    level_rho = rho * weight\n",
        "    print(f\"  {level.capitalize()}: {weight:.0%} ‚Üí œÅ = {level_rho:.4f}\")\n",
        "\n",
        "print(f\"\\nüìã Query Budget Split:\")\n",
        "for query, weight in config.privacy.query_split.items():\n",
        "    query_rho = rho * weight\n",
        "    print(f\"  {query}: {weight:.0%} ‚Üí œÅ = {query_rho:.4f}\")\n",
        "\n",
        "print(f\"\\nüîß Other Settings:\")\n",
        "print(f\"  Contribution Bound Method: {config.privacy.contribution_bound_method}\")\n",
        "print(f\"  Suppression Threshold: {config.privacy.suppression_threshold}\")\n",
        "print(f\"  Sensitivity Method: {config.privacy.sensitivity_method}\")\n",
        "print(f\"  MCC Grouping: {'Enabled' if config.privacy.mcc_grouping_enabled else 'Disabled'}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Configuration validated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Run DP Pipeline\n",
        "\n",
        "Execute the differential privacy pipeline with Top-Down Algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from core.pipeline import DPPipeline\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"EXECUTING DP PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Create and run pipeline\n",
        "pipeline = DPPipeline(config)\n",
        "result = pipeline.run()\n",
        "\n",
        "end_time = datetime.now()\n",
        "duration = (end_time - start_time).total_seconds()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PIPELINE RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if result['success']:\n",
        "    print(f\"\\n‚úÖ SUCCESS!\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå FAILED!\")\n",
        "\n",
        "print(f\"\\nüìä Execution Summary:\")\n",
        "print(f\"  Records Processed: {result.get('total_records', 'N/A'):,}\")\n",
        "print(f\"  Privacy Budget Used: œÅ = {result.get('budget_used', 'N/A')}\")\n",
        "print(f\"  Duration: {duration:.2f} seconds\")\n",
        "print(f\"  Output Path: {result.get('output_path', 'N/A')}\")\n",
        "\n",
        "if result.get('errors'):\n",
        "    print(f\"\\n‚ö†Ô∏è Errors:\")\n",
        "    for error in result['errors']:\n",
        "        print(f\"    - {error}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Privacy Verification\n",
        "\n",
        "Verify that privacy guarantees are correctly implemented.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"PRIVACY GUARANTEE VERIFICATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if not result['success']:\n",
        "    print(\"‚ö†Ô∏è Pipeline failed - skipping privacy verification\")\n",
        "else:\n",
        "    import json\n",
        "    \n",
        "    # Load metadata\n",
        "    output_path = config.data.output_path\n",
        "    metadata_path = os.path.join(output_path, 'metadata.json')\n",
        "    \n",
        "    if os.path.exists(metadata_path):\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "        print(f\"\\nüìã Pipeline Metadata:\")\n",
        "        print(json.dumps(metadata, indent=2))\n",
        "    \n",
        "    # Verify budget composition\n",
        "    print(f\"\\nüîê Budget Composition Verification:\")\n",
        "    total_rho = float(config.privacy.total_rho)\n",
        "    print(f\"  Total Budget: œÅ = {total_rho}\")\n",
        "    \n",
        "    # Geographic composition\n",
        "    geo_rho_sum = sum(total_rho * w for w in config.privacy.geographic_split.values())\n",
        "    print(f\"  Geographic Split Sum: {geo_rho_sum:.4f} (should = {total_rho})\")\n",
        "    geo_check = abs(geo_rho_sum - total_rho) < 1e-6\n",
        "    print(f\"  Geographic Composition: {'‚úÖ VALID' if geo_check else '‚ùå INVALID'}\")\n",
        "    \n",
        "    # Query composition\n",
        "    query_sum = sum(config.privacy.query_split.values())\n",
        "    print(f\"  Query Split Sum: {query_sum:.4f} (should = 1.0)\")\n",
        "    query_check = abs(query_sum - 1.0) < 1e-6\n",
        "    print(f\"  Query Composition: {'‚úÖ VALID' if query_check else '‚ùå INVALID'}\")\n",
        "    \n",
        "    # Sensitivity verification\n",
        "    print(f\"\\nüéØ Sensitivity Verification:\")\n",
        "    # Get computed values, with fallback if not defined (from cell 10)\n",
        "    try:\n",
        "        computed_m = COMPUTED_M\n",
        "        computed_d_max = COMPUTED_D_MAX\n",
        "        computed_k = COMPUTED_K\n",
        "    except NameError:\n",
        "        print(\"  ‚ö†Ô∏è Warning: COMPUTED_M, COMPUTED_D_MAX, or COMPUTED_K not found.\")\n",
        "        print(\"  Using values from config or defaults.\")\n",
        "        computed_m = getattr(config.privacy, 'computed_m', None)\n",
        "        computed_d_max = config.privacy.computed_d_max\n",
        "        computed_k = config.privacy.computed_contribution_bound\n",
        "        if computed_m is None or computed_d_max is None or computed_k is None:\n",
        "            print(\"  ‚ö†Ô∏è Cannot compute sensitivities - missing required parameters.\")\n",
        "            computed_m = 1  # Default fallback\n",
        "            computed_d_max = 1\n",
        "            computed_k = 1\n",
        "    \n",
        "    d_max = config.privacy.computed_d_max or computed_d_max\n",
        "    k_bound = config.privacy.computed_contribution_bound or computed_k\n",
        "    \n",
        "    print(f\"  D_max (max days): {d_max}\")\n",
        "    print(f\"  K (contribution bound): {k_bound}\")\n",
        "    print(f\"  M (max cells): {computed_m}\")\n",
        "    \n",
        "    sqrt_md = math.sqrt(computed_m * d_max)\n",
        "    sens_count = sqrt_md * k_bound\n",
        "    sens_unique = sqrt_md * 1\n",
        "    \n",
        "    print(f\"\\n  Expected Sensitivities (L2):\")\n",
        "    print(f\"    transaction_count: ‚àö(M√óD_max)√óK = {sens_count:.2f}\")\n",
        "    print(f\"    unique_cards: ‚àö(M√óD_max)√ó1 = {sens_unique:.2f}\")\n",
        "    \n",
        "    # Privacy guarantee summary\n",
        "    # Recompute epsilon and delta if not already defined (from cell 12)\n",
        "    try:\n",
        "        _ = epsilon\n",
        "        _ = delta\n",
        "    except NameError:\n",
        "        delta = config.privacy.delta\n",
        "        rho = float(config.privacy.total_rho)\n",
        "        epsilon = rho + 2 * math.sqrt(rho * math.log(1/delta))\n",
        "    \n",
        "    print(f\"\\nüìú PRIVACY GUARANTEE SUMMARY:\")\n",
        "    print(f\"  Mechanism: Discrete Gaussian (zCDP)\")\n",
        "    print(f\"  Privacy Unit: (card_number, month)\")\n",
        "    print(f\"  Composition: Sequential across days, Parallel across cells\")\n",
        "    print(f\"  Total Budget: œÅ = {total_rho} zCDP\")\n",
        "    print(f\"  Equivalent (Œµ,Œ¥)-DP: Œµ ‚âà {epsilon:.2f}, Œ¥ = {delta}\")\n",
        "    \n",
        "    if geo_check and query_check:\n",
        "        print(f\"\\n‚úÖ Privacy verification PASSED!\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Privacy verification FAILED!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. View Results\n",
        "\n",
        "Load and examine the DP-protected output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DP-PROTECTED OUTPUT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "output_path = config.data.output_path\n",
        "\n",
        "if os.path.exists(output_path):\n",
        "    print(f\"\\nüìÅ Output directory: {output_path}\")\n",
        "    print(f\"\\nContents:\")\n",
        "    for item in os.listdir(output_path):\n",
        "        item_path = os.path.join(output_path, item)\n",
        "        if os.path.isfile(item_path):\n",
        "            size = os.path.getsize(item_path)\n",
        "            print(f\"  - {item} ({size:,} bytes)\")\n",
        "        else:\n",
        "            print(f\"  - {item}/\")\n",
        "    \n",
        "    # Load metadata\n",
        "    metadata_path = os.path.join(output_path, 'metadata.json')\n",
        "    if os.path.exists(metadata_path):\n",
        "        print(\"\\nüìã Metadata:\")\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "        print(json.dumps(metadata, indent=2))\n",
        "    \n",
        "    # Load protected data\n",
        "    protected_data_path = os.path.join(output_path, \"protected_data\")\n",
        "    if os.path.exists(protected_data_path):\n",
        "        print(f\"\\nüìä Loading protected data...\")\n",
        "        dp_df = spark.read.parquet(protected_data_path)\n",
        "        dp_count = dp_df.count()\n",
        "        print(f\"  Protected cells: {dp_count:,}\")\n",
        "        print(f\"\\n  Schema:\")\n",
        "        dp_df.printSchema()\n",
        "        print(f\"\\n  Sample:\")\n",
        "        show_df(dp_df, n=10)\n",
        "else:\n",
        "    print(f\"‚ùå Output directory not found: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Utility Evaluation\n",
        "\n",
        "Compare original vs DP-protected data to measure utility loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "from pyspark.sql.functions import col, count, countDistinct, sum as spark_sum\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"UTILITY EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if not result['success']:\n",
        "    print(\"‚ö†Ô∏è Pipeline failed - skipping utility evaluation\")\n",
        "else:\n",
        "    # Aggregate original data to same granularity\n",
        "    print(\"\\nüìä Aggregating original data...\")\n",
        "    original_agg = df_spark.groupBy('city', 'mcc', 'transaction_date').agg(\n",
        "        count('*').alias('transaction_count'),\n",
        "        countDistinct('card_number').alias('unique_cards'),\n",
        "        spark_sum('transaction_amount').alias('transaction_amount_sum')\n",
        "    )\n",
        "    \n",
        "    orig_count = original_agg.count()\n",
        "    print(f\"  Original cells: {orig_count:,}\")\n",
        "    \n",
        "    # Load DP data\n",
        "    protected_data_path = os.path.join(output_path, \"protected_data\")\n",
        "    dp_agg = spark.read.parquet(protected_data_path)\n",
        "    dp_count = dp_agg.count()\n",
        "    print(f\"  DP-protected cells: {dp_count:,}\")\n",
        "    \n",
        "    # Compare totals using Spark\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"AGGREGATE LEVEL COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    NUMERIC_COLS = ['transaction_count', 'unique_cards', 'transaction_amount_sum']\n",
        "    \n",
        "    for col_name in NUMERIC_COLS:\n",
        "        orig_total = original_agg.agg(spark_sum(col_name)).collect()[0][0] or 0\n",
        "        dp_total = dp_agg.agg(spark_sum(col_name)).collect()[0][0] or 0\n",
        "        \n",
        "        if orig_total > 0:\n",
        "            error_pct = abs(dp_total - orig_total) / orig_total * 100\n",
        "            status = \"‚úÖ\" if error_pct < 5 else (\"‚ö†Ô∏è\" if error_pct < 15 else \"‚ùå\")\n",
        "        else:\n",
        "            error_pct = 0\n",
        "            status = \"‚ö†Ô∏è\"\n",
        "        \n",
        "        print(f\"\\n{col_name}:\")\n",
        "        print(f\"  Original Total: {orig_total:,.0f}\")\n",
        "        print(f\"  DP Total: {dp_total:,.0f}\")\n",
        "        print(f\"  Error: {error_pct:.2f}% {status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 10. Production Readiness Checklist\n",
        "\n",
        "Verify the system is ready for production deployment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 10.1 Research-Grade DP Validation (Census 2020 Methodology)\n",
        "\n",
        "The following tests validate the DP implementation according to standards used in the US Census 2020 Disclosure Avoidance System:\n",
        "\n",
        "**A. Statistical Accuracy Tests**\n",
        "- Per-cell error distribution analysis\n",
        "- Bias verification (should be ~0 for unbiased mechanisms)\n",
        "- Variance verification against theoretical œÉ¬≤\n",
        "\n",
        "**B. Privacy Guarantee Verification**\n",
        "- Sensitivity computation validation\n",
        "- Noise calibration verification\n",
        "- Composition theorem verification\n",
        "\n",
        "**C. Utility Metrics (Census 2020 Standard)**\n",
        "- Root Mean Square Error (RMSE)\n",
        "- Mean Absolute Error (MAE)\n",
        "- Coefficient of Variation (CV)\n",
        "- Coverage probability for confidence intervals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "RESEARCH-GRADE DP VALIDATION\n",
        "Following US Census 2020 DAS methodology\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from scipy import stats as scipy_stats\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"RESEARCH-GRADE DP VALIDATION (Census 2020 Methodology)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if not result['success']:\n",
        "    print(\"‚ö†Ô∏è Pipeline failed - skipping research validation\")\n",
        "else:\n",
        "    # =========================================================================\n",
        "    # A. PER-CELL ERROR ANALYSIS\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"A. PER-CELL ERROR ANALYSIS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Join original and DP data at cell level\n",
        "    # First, prepare original aggregates with matching schema\n",
        "    original_cells = df_spark.groupBy('city', 'mcc', 'transaction_date').agg(\n",
        "        F.count('*').alias('orig_count'),\n",
        "        F.countDistinct('card_number').alias('orig_unique'),\n",
        "        F.sum('transaction_amount').alias('orig_amount')\n",
        "    )\n",
        "    \n",
        "    # Load DP data\n",
        "    dp_cells = spark.read.parquet(os.path.join(output_path, \"protected_data\"))\n",
        "    \n",
        "    # Rename DP columns for join\n",
        "    dp_renamed = dp_cells.select(\n",
        "        F.col('city').alias('dp_city'),\n",
        "        F.col('mcc').alias('dp_mcc'),\n",
        "        F.col('transaction_date').alias('dp_date'),\n",
        "        F.col('transaction_count').alias('dp_count'),\n",
        "        F.col('unique_cards').alias('dp_unique'),\n",
        "        F.col('transaction_amount_sum').alias('dp_amount')\n",
        "    )\n",
        "    \n",
        "    # Join on cell key\n",
        "    joined = original_cells.join(\n",
        "        dp_renamed,\n",
        "        (original_cells.city == dp_renamed.dp_city) &\n",
        "        (original_cells.mcc == dp_renamed.dp_mcc) &\n",
        "        (original_cells.transaction_date == dp_renamed.dp_date),\n",
        "        \"outer\"\n",
        "    ).fillna(0)\n",
        "    \n",
        "    # Compute errors\n",
        "    errors_df = joined.select(\n",
        "        'city', 'mcc', 'transaction_date',\n",
        "        'orig_count', 'dp_count',\n",
        "        (F.col('dp_count') - F.col('orig_count')).alias('error_count'),\n",
        "        'orig_unique', 'dp_unique',\n",
        "        (F.col('dp_unique') - F.col('orig_unique')).alias('error_unique'),\n",
        "        'orig_amount', 'dp_amount',\n",
        "        (F.col('dp_amount') - F.col('orig_amount')).alias('error_amount')\n",
        "    )\n",
        "    \n",
        "    # Compute error statistics\n",
        "    error_stats = errors_df.agg(\n",
        "        # Count errors\n",
        "        F.count('*').alias('n_cells'),\n",
        "        F.mean('error_count').alias('bias_count'),\n",
        "        F.stddev('error_count').alias('std_count'),\n",
        "        F.expr('percentile_approx(abs(error_count), 0.5)').alias('mae_count'),\n",
        "        F.sqrt(F.mean(F.pow('error_count', 2))).alias('rmse_count'),\n",
        "        # Unique card errors\n",
        "        F.mean('error_unique').alias('bias_unique'),\n",
        "        F.stddev('error_unique').alias('std_unique'),\n",
        "        F.expr('percentile_approx(abs(error_unique), 0.5)').alias('mae_unique'),\n",
        "        F.sqrt(F.mean(F.pow('error_unique', 2))).alias('rmse_unique'),\n",
        "        # Amount errors\n",
        "        F.mean('error_amount').alias('bias_amount'),\n",
        "        F.stddev('error_amount').alias('std_amount'),\n",
        "        F.sqrt(F.mean(F.pow('error_amount', 2))).alias('rmse_amount')\n",
        "    ).collect()[0]\n",
        "    \n",
        "    print(f\"\\nüìä Error Statistics Across {error_stats['n_cells']:,} Cells:\")\n",
        "    print(f\"\\n  TRANSACTION COUNT:\")\n",
        "    print(f\"    Bias (should be ‚âà0): {error_stats['bias_count']:.4f}\")\n",
        "    print(f\"    Std Dev: {error_stats['std_count']:.2f}\")\n",
        "    print(f\"    MAE: {error_stats['mae_count']:.2f}\")\n",
        "    print(f\"    RMSE: {error_stats['rmse_count']:.2f}\")\n",
        "    \n",
        "    print(f\"\\n  UNIQUE CARDS:\")\n",
        "    print(f\"    Bias (should be ‚âà0): {error_stats['bias_unique']:.4f}\")\n",
        "    print(f\"    Std Dev: {error_stats['std_unique']:.2f}\")\n",
        "    print(f\"    MAE: {error_stats['mae_unique']:.2f}\")\n",
        "    print(f\"    RMSE: {error_stats['rmse_unique']:.2f}\")\n",
        "    \n",
        "    print(f\"\\n  TRANSACTION AMOUNT:\")\n",
        "    print(f\"    Bias (should be ‚âà0): {error_stats['bias_amount']:.2f}\")\n",
        "    print(f\"    Std Dev: {error_stats['std_amount']:.2f}\")\n",
        "    print(f\"    RMSE: {error_stats['rmse_amount']:.2f}\")\n",
        "    \n",
        "    # Bias test (should not reject H0: bias=0)\n",
        "    n_cells = error_stats['n_cells']\n",
        "    bias_count = error_stats['bias_count']\n",
        "    std_count = error_stats['std_count']\n",
        "    \n",
        "    if std_count > 0 and n_cells > 30:\n",
        "        t_stat = bias_count / (std_count / np.sqrt(n_cells))\n",
        "        p_value = 2 * (1 - scipy_stats.t.cdf(abs(t_stat), n_cells - 1))\n",
        "        bias_test_pass = p_value > 0.05\n",
        "        print(f\"\\n  üìà Bias Test (H0: bias=0):\")\n",
        "        print(f\"    t-statistic: {t_stat:.4f}\")\n",
        "        print(f\"    p-value: {p_value:.4f}\")\n",
        "        print(f\"    Result: {'‚úÖ PASS (unbiased)' if bias_test_pass else '‚ùå FAIL (biased)'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 12. Interactive 3D Visualization of DP Noise\n",
        "\n",
        "Scientific-level visualization of differential privacy noise effects using interactive 3D surface plots.\n",
        "\n",
        "**Features:**\n",
        "- **Dual Surface Plots**: Side-by-side comparison of original vs DP-protected data\n",
        "- **Dynamic Axes**: User-configurable X/Y axes from (City, MCC, Day)\n",
        "- **Province/Month Filtering**: Select specific province and month for analysis\n",
        "- **Metric Selection**: Visualize any of the three queries\n",
        "- **Statistical Metrics**: RMSE, MAE, and maximum error displayed\n",
        "- **Publication Quality**: Suitable for research papers and presentations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "INTERACTIVE 3D VISUALIZATION OF DIFFERENTIAL PRIVACY NOISE\n",
        "===========================================================\n",
        "Scientific-level visualization using Plotly for publication-quality figures.\n",
        "\"\"\"\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ipywidgets import widgets, interactive\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"3D VISUALIZATION: ORIGINAL vs DP-PROTECTED DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if not result['success']:\n",
        "    print(\"‚ö†Ô∏è Pipeline failed - cannot create visualization\")\n",
        "else:\n",
        "    # =========================================================================\n",
        "    # STEP 1: DATA PREPARATION\n",
        "    # =========================================================================\n",
        "    print(\"\\nüìä Preparing data for visualization...\")\n",
        "    \n",
        "    # Load original data with province info\n",
        "    from schema.geography import Geography\n",
        "    \n",
        "    geo = Geography.from_csv(CITY_PROVINCE_PATH)\n",
        "    \n",
        "    # Create broadcast mapping for province lookup\n",
        "    city_to_province_map = geo.city_to_province_broadcast()\n",
        "    \n",
        "    # Aggregate original data to cell level\n",
        "    original_cells = df_spark.groupBy('city', 'mcc', 'transaction_date').agg(\n",
        "        F.count('*').alias('orig_transaction_count'),\n",
        "        F.countDistinct('card_number').alias('orig_unique_cards'),\n",
        "        F.sum('transaction_amount').alias('orig_total_amount')\n",
        "    )\n",
        "    \n",
        "    # Add province information to original cells\n",
        "    @F.udf('int')\n",
        "    def get_province_code(city):\n",
        "        if city in city_to_province_map:\n",
        "            return city_to_province_map[city][0]  # province_code\n",
        "        return geo.UNKNOWN_PROVINCE_CODE  # Unknown\n",
        "    \n",
        "    @F.udf('int')\n",
        "    def get_city_code(city):\n",
        "        if city in city_to_province_map:\n",
        "            return city_to_province_map[city][2]  # city_code\n",
        "        return geo.UNKNOWN_CITY_CODE  # Unknown\n",
        "    \n",
        "    original_cells = original_cells.withColumn('province_code', get_province_code('city'))\n",
        "    original_cells = original_cells.withColumn('city_code', get_city_code('city'))\n",
        "    \n",
        "    # Load DP-protected data\n",
        "    dp_cells = spark.read.parquet(os.path.join(output_path, \"protected_data\"))\n",
        "    \n",
        "    # Convert dates to day indices for both\n",
        "    # Extract day index from transaction_date (assuming it's 0-based day index or date string)\n",
        "    original_cells = original_cells.withColumn(\n",
        "        'day_idx',\n",
        "        F.when(F.col('transaction_date').cast('int').isNotNull(), \n",
        "               F.col('transaction_date').cast('int'))\n",
        "        .otherwise(F.datediff(F.col('transaction_date'), F.min('transaction_date').over(F.Window.orderBy())))\n",
        "    )\n",
        "    \n",
        "    # Join original and DP data\n",
        "    joined_data = original_cells.join(\n",
        "        dp_cells,\n",
        "        (original_cells.province_code == dp_cells.province_code) &\n",
        "        (original_cells.city_code == dp_cells.city_code) &\n",
        "        (original_cells.mcc == dp_cells.mcc) &\n",
        "        (original_cells.day_idx == dp_cells.day_idx),\n",
        "        \"outer\"\n",
        "    ).fillna(0)\n",
        "    \n",
        "    # Convert to Pandas for visualization (should be manageable size after aggregation)\n",
        "    viz_df = joined_data.select(\n",
        "        F.coalesce(original_cells.province_code, dp_cells.province_code).alias('province_code'),\n",
        "        F.coalesce(original_cells.city_code, dp_cells.city_code).alias('city_code'),\n",
        "        F.coalesce(original_cells.mcc, dp_cells.mcc).alias('mcc'),\n",
        "        F.coalesce(original_cells.day_idx, dp_cells.day_idx).alias('day_idx'),\n",
        "        'orig_transaction_count',\n",
        "        'orig_unique_cards',\n",
        "        'orig_total_amount',\n",
        "        'transaction_count',\n",
        "        'unique_cards',\n",
        "        'transaction_amount_sum'\n",
        "    ).toPandas()\n",
        "    \n",
        "    # Rename DP columns for consistency\n",
        "    viz_df.rename(columns={\n",
        "        'transaction_count': 'dp_transaction_count',\n",
        "        'unique_cards': 'dp_unique_cards',\n",
        "        'transaction_amount_sum': 'dp_total_amount'\n",
        "    }, inplace=True)\n",
        "    \n",
        "    print(f\"‚úÖ Data prepared: {len(viz_df):,} cells loaded\")\n",
        "    \n",
        "    # Get unique values for filters\n",
        "    provinces = sorted(viz_df['province_code'].unique())\n",
        "    cities = sorted(viz_df['city_code'].unique())\n",
        "    mccs = sorted(viz_df['mcc'].unique())\n",
        "    days = sorted(viz_df['day_idx'].unique())\n",
        "    \n",
        "    print(f\"  Provinces: {len(provinces)}\")\n",
        "    print(f\"  Cities: {len(cities)}\")\n",
        "    print(f\"  MCCs: {len(mccs)}\")\n",
        "    print(f\"  Days: {len(days)}\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 2: VISUALIZATION FUNCTION\n",
        "    # =========================================================================\n",
        "    \n",
        "    def create_3d_surface_comparison(\n",
        "        df, \n",
        "        x_axis='day_idx', \n",
        "        y_axis='mcc', \n",
        "        metric='transaction_count',\n",
        "        province_filter=None,\n",
        "        month_filter=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create side-by-side 3D surface plots comparing original and DP-protected data.\n",
        "        \n",
        "        Args:\n",
        "            df: DataFrame with joined original and DP data\n",
        "            x_axis: Column name for X-axis ('city_code', 'mcc', 'day_idx')\n",
        "            y_axis: Column name for Y-axis ('city_code', 'mcc', 'day_idx')\n",
        "            metric: Metric to visualize ('transaction_count', 'unique_cards', 'total_amount')\n",
        "            province_filter: Province code to filter (None = all provinces)\n",
        "            month_filter: Month to filter (None = all months)\n",
        "        \n",
        "        Returns:\n",
        "            Plotly figure object\n",
        "        \"\"\"\n",
        "        # Filter data\n",
        "        filtered_df = df.copy()\n",
        "        if province_filter is not None:\n",
        "            filtered_df = filtered_df[filtered_df['province_code'] == province_filter]\n",
        "        \n",
        "        # Determine aggregation dimension (the one not used for axes)\n",
        "        all_dims = {'city_code', 'mcc', 'day_idx'}\n",
        "        used_dims = {x_axis, y_axis}\n",
        "        agg_dim = list(all_dims - used_dims)[0]\n",
        "        \n",
        "        # Aggregate over the unused dimension\n",
        "        orig_col = f'orig_{metric}'\n",
        "        dp_col = f'dp_{metric}'\n",
        "        \n",
        "        grouped = filtered_df.groupby([x_axis, y_axis]).agg({\n",
        "            orig_col: 'sum',\n",
        "            dp_col: 'sum'\n",
        "        }).reset_index()\n",
        "        \n",
        "        # Create pivot tables for surface plots\n",
        "        pivot_orig = grouped.pivot(index=y_axis, columns=x_axis, values=orig_col)\n",
        "        pivot_dp = grouped.pivot(index=y_axis, columns=x_axis, values=dp_col)\n",
        "        \n",
        "        # Fill missing values with NaN for proper visualization\n",
        "        pivot_orig = pivot_orig.fillna(0)\n",
        "        pivot_dp = pivot_dp.fillna(0)\n",
        "        \n",
        "        # Convert to numpy arrays\n",
        "        z_orig = pivot_orig.values\n",
        "        z_dp = pivot_dp.values\n",
        "        x_vals = pivot_orig.columns.values\n",
        "        y_vals = pivot_orig.index.values\n",
        "        \n",
        "        # Compute error metrics\n",
        "        valid_mask = (z_orig > 0) | (z_dp > 0)\n",
        "        if valid_mask.sum() > 0:\n",
        "            errors = z_dp[valid_mask] - z_orig[valid_mask]\n",
        "            rmse = np.sqrt(np.mean(errors**2))\n",
        "            mae = np.mean(np.abs(errors))\n",
        "            max_error = np.max(np.abs(errors))\n",
        "            bias = np.mean(errors)\n",
        "            \n",
        "            # Relative error for non-zero cells\n",
        "            nonzero_mask = z_orig[valid_mask] > 0\n",
        "            if nonzero_mask.sum() > 0:\n",
        "                rel_errors = np.abs(errors[nonzero_mask]) / z_orig[valid_mask][nonzero_mask] * 100\n",
        "                mean_rel_error = np.mean(rel_errors)\n",
        "            else:\n",
        "                mean_rel_error = 0\n",
        "        else:\n",
        "            rmse = mae = max_error = bias = mean_rel_error = 0\n",
        "        \n",
        "        # Create subplot figure (1 row, 2 columns)\n",
        "        fig = make_subplots(\n",
        "            rows=1, cols=2,\n",
        "            subplot_titles=('Original Data', 'DP-Protected Data'),\n",
        "            specs=[[{'type': 'surface'}, {'type': 'surface'}]],\n",
        "            horizontal_spacing=0.05\n",
        "        )\n",
        "        \n",
        "        # Determine color scale range (use same for both plots)\n",
        "        vmin = min(z_orig.min(), z_dp.min())\n",
        "        vmax = max(z_orig.max(), z_dp.max())\n",
        "        \n",
        "        # Color scale: professional scientific palette\n",
        "        colorscale = 'Viridis'  # or 'Plasma', 'Inferno', 'Turbo'\n",
        "        \n",
        "        # Original data surface\n",
        "        fig.add_trace(\n",
        "            go.Surface(\n",
        "                z=z_orig,\n",
        "                x=x_vals,\n",
        "                y=y_vals,\n",
        "                colorscale=colorscale,\n",
        "                cmin=vmin,\n",
        "                cmax=vmax,\n",
        "                showscale=False,\n",
        "                hovertemplate=(\n",
        "                    f'{x_axis}: %{{x}}<br>'\n",
        "                    f'{y_axis}: %{{y}}<br>'\n",
        "                    'Value: %{z:,.0f}<br>'\n",
        "                    '<extra></extra>'\n",
        "                ),\n",
        "                name='Original'\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # DP-protected data surface\n",
        "        fig.add_trace(\n",
        "            go.Surface(\n",
        "                z=z_dp,\n",
        "                x=x_vals,\n",
        "                y=y_vals,\n",
        "                colorscale=colorscale,\n",
        "                cmin=vmin,\n",
        "                cmax=vmax,\n",
        "                showscale=True,\n",
        "                colorbar=dict(\n",
        "                    title=metric.replace('_', ' ').title(),\n",
        "                    x=1.02\n",
        "                ),\n",
        "                hovertemplate=(\n",
        "                    f'{x_axis}: %{{x}}<br>'\n",
        "                    f'{y_axis}: %{{y}}<br>'\n",
        "                    'Value: %{z:,.0f}<br>'\n",
        "                    '<extra></extra>'\n",
        "                ),\n",
        "                name='DP-Protected'\n",
        "            ),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # Update layout\n",
        "        axis_labels = {\n",
        "            'city_code': 'City Code',\n",
        "            'mcc': 'MCC',\n",
        "            'day_idx': 'Day Index'\n",
        "        }\n",
        "        \n",
        "        title_parts = [f'3D Surface: {metric.replace(\"_\", \" \").title()}']\n",
        "        if province_filter is not None:\n",
        "            title_parts.append(f'Province {province_filter}')\n",
        "        title_parts.append(f'Aggregated over {agg_dim.replace(\"_\", \" \")}')\n",
        "        title_parts.append(f'(œÅ={float(config.privacy.total_rho):.3f})')\n",
        "        \n",
        "        fig.update_layout(\n",
        "            title=dict(\n",
        "                text=' | '.join(title_parts),\n",
        "                font=dict(size=14)\n",
        "            ),\n",
        "            scene=dict(\n",
        "                xaxis_title=axis_labels.get(x_axis, x_axis),\n",
        "                yaxis_title=axis_labels.get(y_axis, y_axis),\n",
        "                zaxis_title='Value',\n",
        "                camera=dict(eye=dict(x=1.5, y=1.5, z=1.3))\n",
        "            ),\n",
        "            scene2=dict(\n",
        "                xaxis_title=axis_labels.get(x_axis, x_axis),\n",
        "                yaxis_title=axis_labels.get(y_axis, y_axis),\n",
        "                zaxis_title='Value',\n",
        "                camera=dict(eye=dict(x=1.5, y=1.5, z=1.3))\n",
        "            ),\n",
        "            height=600,\n",
        "            width=1400,\n",
        "            showlegend=False\n",
        "        )\n",
        "        \n",
        "        # Add statistical annotation\n",
        "        annotation_text = (\n",
        "            f'<b>Statistical Metrics:</b><br>'\n",
        "            f'RMSE: {rmse:,.2f} | '\n",
        "            f'MAE: {mae:,.2f} | '\n",
        "            f'Max Error: {max_error:,.0f} | '\n",
        "            f'Bias: {bias:,.2f} | '\n",
        "            f'Mean Rel. Error: {mean_rel_error:.1f}%<br>'\n",
        "            f'Cells: {valid_mask.sum():,} | '\n",
        "            f'Province-Month Total: EXACT (public invariant)'\n",
        "        )\n",
        "        \n",
        "        fig.add_annotation(\n",
        "            text=annotation_text,\n",
        "            xref=\"paper\", yref=\"paper\",\n",
        "            x=0.5, y=-0.05,\n",
        "            showarrow=False,\n",
        "            font=dict(size=11),\n",
        "            align='center',\n",
        "            xanchor='center'\n",
        "        )\n",
        "        \n",
        "        return fig\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 3: INTERACTIVE CONTROLS\n",
        "    # =========================================================================\n",
        "    \n",
        "    print(\"\\nüéõÔ∏è Creating interactive controls...\")\n",
        "    \n",
        "    # Metric mapping\n",
        "    metric_options = {\n",
        "        'Transaction Count': 'transaction_count',\n",
        "        'Unique Cards': 'unique_cards',\n",
        "        'Total Amount': 'total_amount'\n",
        "    }\n",
        "    \n",
        "    # Axis options\n",
        "    axis_options = {\n",
        "        'Day Index': 'day_idx',\n",
        "        'MCC (Merchant Category)': 'mcc',\n",
        "        'City Code': 'city_code'\n",
        "    }\n",
        "    \n",
        "    # Create widgets\n",
        "    x_axis_widget = widgets.Dropdown(\n",
        "        options=list(axis_options.keys()),\n",
        "        value='Day Index',\n",
        "        description='X-Axis:',\n",
        "        style={'description_width': '120px'}\n",
        "    )\n",
        "    \n",
        "    y_axis_widget = widgets.Dropdown(\n",
        "        options=list(axis_options.keys()),\n",
        "        value='MCC (Merchant Category)',\n",
        "        description='Y-Axis:',\n",
        "        style={'description_width': '120px'}\n",
        "    )\n",
        "    \n",
        "    metric_widget = widgets.Dropdown(\n",
        "        options=list(metric_options.keys()),\n",
        "        value='Transaction Count',\n",
        "        description='Metric:',\n",
        "        style={'description_width': '120px'}\n",
        "    )\n",
        "    \n",
        "    province_widget = widgets.Dropdown(\n",
        "        options=[('All Provinces', None)] + [(f'Province {p}', p) for p in provinces],\n",
        "        value=None,\n",
        "        description='Province:',\n",
        "        style={'description_width': '120px'}\n",
        "    )\n",
        "    \n",
        "    # Update button\n",
        "    update_button = widgets.Button(\n",
        "        description='Update Visualization',\n",
        "        button_style='primary',\n",
        "        icon='refresh'\n",
        "    )\n",
        "    \n",
        "    # Output widget for the plot\n",
        "    output_widget = widgets.Output()\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 4: UPDATE FUNCTION\n",
        "    # =========================================================================\n",
        "    \n",
        "    def update_plot(b=None):\n",
        "        \"\"\"Update the 3D visualization based on widget selections.\"\"\"\n",
        "        with output_widget:\n",
        "            output_widget.clear_output(wait=True)\n",
        "            \n",
        "            # Get selected values\n",
        "            x_axis_name = x_axis_widget.value\n",
        "            y_axis_name = y_axis_widget.value\n",
        "            metric_name = metric_widget.value\n",
        "            province_val = province_widget.value\n",
        "            \n",
        "            x_axis = axis_options[x_axis_name]\n",
        "            y_axis = axis_options[y_axis_name]\n",
        "            metric = metric_options[metric_name]\n",
        "            \n",
        "            # Validate axes are different\n",
        "            if x_axis == y_axis:\n",
        "                print(\"‚ö†Ô∏è X-axis and Y-axis must be different. Please select different axes.\")\n",
        "                return\n",
        "            \n",
        "            # Create and display figure\n",
        "            try:\n",
        "                fig = create_3d_surface_comparison(\n",
        "                    viz_df,\n",
        "                    x_axis=x_axis,\n",
        "                    y_axis=y_axis,\n",
        "                    metric=metric,\n",
        "                    province_filter=province_val\n",
        "                )\n",
        "                fig.show()\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error creating visualization: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "    \n",
        "    update_button.on_click(update_plot)\n",
        "    \n",
        "    # =========================================================================\n",
        "    # STEP 5: DISPLAY INTERFACE\n",
        "    # =========================================================================\n",
        "    \n",
        "    print(\"‚úÖ Visualization ready!\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"INTERACTIVE CONTROLS\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Configure the visualization parameters below and click 'Update Visualization'\")\n",
        "    print(\"\\nNote: The third dimension (not selected for X or Y) will be aggregated.\")\n",
        "    print(\"Province-month totals are EXACT (public data) - noise is at cell level.\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Display controls\n",
        "    display(HTML(\"<h3>Visualization Configuration</h3>\"))\n",
        "    display(widgets.VBox([\n",
        "        widgets.HBox([x_axis_widget, y_axis_widget]),\n",
        "        widgets.HBox([metric_widget, province_widget]),\n",
        "        update_button\n",
        "    ]))\n",
        "    \n",
        "    # Display output area\n",
        "    display(output_widget)\n",
        "    \n",
        "    # Create initial plot\n",
        "    print(\"\\nüìä Generating initial visualization...\")\n",
        "    update_plot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    # =========================================================================\n",
        "    # B. PRIVACY GUARANTEE VERIFICATION\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"B. PRIVACY GUARANTEE VERIFICATION\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Get privacy parameters\n",
        "    total_rho = float(config.privacy.total_rho)\n",
        "    delta = config.privacy.delta\n",
        "    \n",
        "    # Compute theoretical noise parameters\n",
        "    # For zCDP with œÅ, the Gaussian mechanism uses œÉ¬≤ = Œî¬≤/(2œÅ)\n",
        "    \n",
        "    # Get sensitivity values (from preprocessing or computed)\n",
        "    try:\n",
        "        d_max_val = config.privacy.computed_d_max or COMPUTED_D_MAX\n",
        "        k_val = config.privacy.computed_contribution_bound or COMPUTED_K\n",
        "        m_val = COMPUTED_M\n",
        "    except NameError:\n",
        "        d_max_val = config.privacy.computed_d_max or 1\n",
        "        k_val = config.privacy.computed_contribution_bound or 1\n",
        "        m_val = 1\n",
        "    \n",
        "    # L2 sensitivity for count query: sqrt(M * D_max) * K\n",
        "    l2_sens_count = np.sqrt(m_val * d_max_val) * k_val\n",
        "    l2_sens_unique = np.sqrt(m_val * d_max_val) * 1  # Each card contributes 1\n",
        "    \n",
        "    # Budget per query (assuming equal split for simplicity)\n",
        "    rho_per_query = total_rho * config.privacy.query_split.get('transaction_count', 0.34)\n",
        "    rho_per_query_city = rho_per_query * config.privacy.geographic_split.get('city', 0.8)\n",
        "    \n",
        "    # Theoretical variance: œÉ¬≤ = Œî¬≤/(2œÅ)\n",
        "    theoretical_var_count = (l2_sens_count ** 2) / (2 * rho_per_query_city)\n",
        "    theoretical_std_count = np.sqrt(theoretical_var_count)\n",
        "    \n",
        "    print(f\"\\nüîê Privacy Parameters:\")\n",
        "    print(f\"  Total œÅ (zCDP): {total_rho}\")\n",
        "    print(f\"  Œ¥: {delta}\")\n",
        "    print(f\"  Œµ (converted): {total_rho + 2 * np.sqrt(total_rho * np.log(1/delta)):.2f}\")\n",
        "    \n",
        "    print(f\"\\nüéØ Sensitivity Analysis:\")\n",
        "    print(f\"  M (max cells per card): {m_val}\")\n",
        "    print(f\"  D_max (max days per card): {d_max_val}\")\n",
        "    print(f\"  K (contribution bound): {k_val}\")\n",
        "    print(f\"  L2 Sensitivity (count): {l2_sens_count:.2f}\")\n",
        "    print(f\"  L2 Sensitivity (unique): {l2_sens_unique:.2f}\")\n",
        "    \n",
        "    print(f\"\\nüìä Noise Calibration Verification:\")\n",
        "    print(f\"  œÅ per query (city level): {rho_per_query_city:.6f}\")\n",
        "    print(f\"  Theoretical œÉ (count): {theoretical_std_count:.2f}\")\n",
        "    print(f\"  Observed œÉ (count): {error_stats['std_count']:.2f}\")\n",
        "    \n",
        "    # Check if observed variance is close to theoretical\n",
        "    # Allow 50% tolerance due to post-processing (NNLS, rounding)\n",
        "    var_ratio = error_stats['std_count'] / theoretical_std_count if theoretical_std_count > 0 else float('inf')\n",
        "    var_check = 0.5 <= var_ratio <= 2.0\n",
        "    \n",
        "    print(f\"  Ratio (observed/theoretical): {var_ratio:.2f}\")\n",
        "    print(f\"  Variance Check: {'‚úÖ PASS' if var_check else '‚ö†Ô∏è WARNING (post-processing may affect variance)'}\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # C. COMPOSITION VERIFICATION\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"C. COMPOSITION THEOREM VERIFICATION\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # zCDP composition: œÅ_total = Œ£ œÅ_i (additive)\n",
        "    geo_weights = config.privacy.geographic_split\n",
        "    query_weights = config.privacy.query_split\n",
        "    \n",
        "    print(f\"\\nüìê Budget Composition:\")\n",
        "    print(f\"  Geographic levels: {list(geo_weights.keys())}\")\n",
        "    print(f\"  Queries: {list(query_weights.keys())}\")\n",
        "    \n",
        "    # Verify weights sum to 1\n",
        "    geo_sum = sum(geo_weights.values())\n",
        "    query_sum = sum(query_weights.values())\n",
        "    \n",
        "    print(f\"\\n  Geographic weights sum: {geo_sum:.4f} (should = 1.0)\")\n",
        "    print(f\"  Query weights sum: {query_sum:.4f} (should = 1.0)\")\n",
        "    \n",
        "    # Total budget breakdown\n",
        "    print(f\"\\n  Budget Allocation:\")\n",
        "    for geo_level, geo_w in geo_weights.items():\n",
        "        for query, query_w in query_weights.items():\n",
        "            allocated_rho = total_rho * geo_w * query_w\n",
        "            print(f\"    {geo_level}/{query}: œÅ = {allocated_rho:.6f}\")\n",
        "    \n",
        "    # Verify total\n",
        "    total_allocated = sum(\n",
        "        total_rho * geo_w * query_w \n",
        "        for geo_w in geo_weights.values() \n",
        "        for query_w in query_weights.values()\n",
        "    )\n",
        "    composition_valid = abs(total_allocated - total_rho) < 1e-10\n",
        "    \n",
        "    print(f\"\\n  Total allocated: œÅ = {total_allocated:.6f}\")\n",
        "    print(f\"  Original budget: œÅ = {total_rho:.6f}\")\n",
        "    print(f\"  Composition: {'‚úÖ VALID' if composition_valid else '‚ùå INVALID'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    # =========================================================================\n",
        "    # D. UTILITY BY COUNT SIZE (Census 2020 Style Analysis)\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"D. UTILITY BY COUNT SIZE (Census 2020 Analysis)\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Stratify by original count size\n",
        "    stratified = errors_df.withColumn(\n",
        "        'count_bucket',\n",
        "        F.when(F.col('orig_count') == 0, '0 (empty)')\n",
        "        .when(F.col('orig_count') <= 5, '1-5 (small)')\n",
        "        .when(F.col('orig_count') <= 20, '6-20 (medium)')\n",
        "        .when(F.col('orig_count') <= 100, '21-100 (large)')\n",
        "        .otherwise('>100 (very large)')\n",
        "    )\n",
        "    \n",
        "    bucket_stats = stratified.groupBy('count_bucket').agg(\n",
        "        F.count('*').alias('n_cells'),\n",
        "        F.mean('error_count').alias('mean_error'),\n",
        "        F.stddev('error_count').alias('std_error'),\n",
        "        F.mean(F.abs('error_count')).alias('mae'),\n",
        "        F.mean(\n",
        "            F.when(F.col('orig_count') > 0, \n",
        "                   F.abs(F.col('error_count')) / F.col('orig_count') * 100)\n",
        "            .otherwise(None)\n",
        "        ).alias('mape')\n",
        "    ).orderBy('count_bucket')\n",
        "    \n",
        "    print(\"\\nüìä Error by Original Count Size:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Bucket':<20} {'N Cells':>10} {'Mean Err':>12} {'Std Err':>12} {'MAE':>10} {'MAPE %':>10}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for row in bucket_stats.collect():\n",
        "        mape_str = f\"{row['mape']:.1f}\" if row['mape'] is not None else \"N/A\"\n",
        "        print(f\"{row['count_bucket']:<20} {row['n_cells']:>10,} {row['mean_error']:>12.2f} \"\n",
        "              f\"{row['std_error']:>12.2f} {row['mae']:>10.2f} {mape_str:>10}\")\n",
        "    \n",
        "    # =========================================================================\n",
        "    # E. RESEARCH READINESS SUMMARY\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"E. RESEARCH READINESS SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    research_checks = []\n",
        "    \n",
        "    # 1. Unbiasedness\n",
        "    bias_ok = abs(error_stats['bias_count']) < 1.0  # Allow small bias\n",
        "    research_checks.append(('Unbiased Mechanism', bias_ok))\n",
        "    \n",
        "    # 2. Variance calibration\n",
        "    research_checks.append(('Variance Calibration', var_check))\n",
        "    \n",
        "    # 3. Composition validity\n",
        "    research_checks.append(('Budget Composition', composition_valid))\n",
        "    \n",
        "    # 4. Reasonable utility (MAPE < 50% for medium+ cells)\n",
        "    medium_plus = stratified.filter(F.col('orig_count') > 5)\n",
        "    if medium_plus.count() > 0:\n",
        "        avg_mape = medium_plus.filter(F.col('orig_count') > 0).agg(\n",
        "            F.mean(F.abs(F.col('error_count')) / F.col('orig_count') * 100)\n",
        "        ).collect()[0][0]\n",
        "        utility_ok = avg_mape is not None and avg_mape < 50\n",
        "        research_checks.append(('Reasonable Utility (MAPE<50%)', utility_ok))\n",
        "    \n",
        "    # 5. No systematic errors\n",
        "    systematic_ok = abs(error_stats['bias_unique']) < 1.0\n",
        "    research_checks.append(('No Systematic Errors', systematic_ok))\n",
        "    \n",
        "    print(\"\\nüìã Research Validation Checklist:\")\n",
        "    for check_name, passed in research_checks:\n",
        "        status = '‚úÖ' if passed else '‚ùå'\n",
        "        print(f\"  {status} {check_name}\")\n",
        "    \n",
        "    all_research_passed = all(c[1] for c in research_checks)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    if all_research_passed:\n",
        "        print(\"üéì RESEARCH READY: This DP implementation passes Census 2020-style validation.\")\n",
        "        print(\"   The methodology is suitable for academic research and publication.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NOT RESEARCH READY: Some validation checks failed.\")\n",
        "        print(\"   Review the failed checks before using for research.\")\n",
        "        failed = [c[0] for c in research_checks if not c[1]]\n",
        "        print(f\"   Failed: {', '.join(failed)}\")\n",
        "    print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"PRODUCTION READINESS CHECKLIST\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "checks = []\n",
        "\n",
        "# 1. Pipeline Success\n",
        "check_1 = result['success']\n",
        "checks.append(('Pipeline Execution', check_1))\n",
        "print(f\"\\n{'‚úÖ' if check_1 else '‚ùå'} Pipeline Execution: {'PASSED' if check_1 else 'FAILED'}\")\n",
        "\n",
        "# 2. Output Files Exist\n",
        "output_exists = os.path.exists(os.path.join(output_path, 'protected_data'))\n",
        "checks.append(('Output Files', output_exists))\n",
        "print(f\"{'‚úÖ' if output_exists else '‚ùå'} Output Files: {'EXIST' if output_exists else 'MISSING'}\")\n",
        "\n",
        "# 3. Metadata Present\n",
        "metadata_exists = os.path.exists(os.path.join(output_path, 'metadata.json'))\n",
        "checks.append(('Metadata', metadata_exists))\n",
        "print(f\"{'‚úÖ' if metadata_exists else '‚ùå'} Metadata: {'PRESENT' if metadata_exists else 'MISSING'}\")\n",
        "\n",
        "# 4. Budget Composition Valid\n",
        "budget_valid = abs(sum(config.privacy.geographic_split.values()) - 1.0) < 1e-6\n",
        "budget_valid = budget_valid and abs(sum(config.privacy.query_split.values()) - 1.0) < 1e-6\n",
        "checks.append(('Budget Composition', budget_valid))\n",
        "print(f\"{'‚úÖ' if budget_valid else '‚ùå'} Budget Composition: {'VALID' if budget_valid else 'INVALID'}\")\n",
        "\n",
        "# 5. No Negative Counts (sanity check)\n",
        "if output_exists:\n",
        "    dp_df = spark.read.parquet(os.path.join(output_path, 'protected_data'))\n",
        "    neg_counts = dp_df.filter(F.col('transaction_count') < 0).count()\n",
        "    no_negative = neg_counts == 0\n",
        "    checks.append(('No Negative Counts', no_negative))\n",
        "    print(f\"{'‚úÖ' if no_negative else '‚ö†Ô∏è'} No Negative Counts: {'PASSED' if no_negative else f'{neg_counts} negative values'}\")\n",
        "\n",
        "# 6. Reasonable Processing Time\n",
        "reasonable_time = duration < 300  # 5 minutes for test data\n",
        "checks.append(('Processing Time', reasonable_time))\n",
        "print(f\"{'‚úÖ' if reasonable_time else '‚ö†Ô∏è'} Processing Time: {duration:.1f}s {'(OK)' if reasonable_time else '(SLOW)'}\")\n",
        "\n",
        "# Summary\n",
        "all_passed = all(c[1] for c in checks)\n",
        "passed_count = sum(1 for c in checks if c[1])\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"SUMMARY: {passed_count}/{len(checks)} checks passed\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if all_passed:\n",
        "    print(f\"\\nüéâ PRODUCTION READY!\")\n",
        "    print(f\"   The DP system has passed all checks and is ready for deployment.\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è NOT READY FOR PRODUCTION\")\n",
        "    print(f\"   Please address the failed checks before deployment.\")\n",
        "    failed = [c[0] for c in checks if not c[1]]\n",
        "    print(f\"   Failed: {', '.join(failed)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 11. Cleanup & Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to clean up generated output files\n",
        "# import shutil\n",
        "# \n",
        "# if os.path.exists(config.data.output_path):\n",
        "#     shutil.rmtree(config.data.output_path)\n",
        "#     print(f\"Removed: {config.data.output_path}\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"NOTEBOOK COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTimestamp: {datetime.now().isoformat()}\")\n",
        "print(f\"\\nüìã Summary:\")\n",
        "print(f\"  - Records processed: {result.get('total_records', 'N/A'):,}\")\n",
        "print(f\"  - Privacy budget: œÅ = {config.privacy.total_rho}\")\n",
        "print(f\"  - Pipeline status: {'‚úÖ SUCCESS' if result['success'] else '‚ùå FAILED'}\")\n",
        "\n",
        "# Check if all_passed is defined (from cell 22)\n",
        "try:\n",
        "    production_status = '‚úÖ YES' if all_passed else '‚ùå NO'\n",
        "    print(f\"  - Production ready: {production_status}\")\n",
        "except NameError:\n",
        "    print(f\"  - Production ready: ‚ö†Ô∏è Run cell 22 to check production readiness\")\n",
        "\n",
        "print(f\"\\nüìä Output Metrics (with DP):\")\n",
        "print(f\"  - transaction_count: Count of transactions per (date, city, mcc)\")\n",
        "print(f\"  - unique_cards: Count of distinct cards per (date, city, mcc)\")\n",
        "print(f\"  - transaction_amount_sum: Sum of amounts per (date, city, mcc)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
